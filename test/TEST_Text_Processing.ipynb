{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.CoNLL_Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_conll_file1 = '../data/conll_test_files/Alliance114418822/Alliance114418822_fold1.conll'\n",
    "path_test_conll_file2 = '../data/conll_test_files/stanford_ner_without_start_without_empty_space.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alliance114418822\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/conll_test_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At    O\r\n",
      "the    O\r\n",
      "same    O\r\n",
      "time    O\r\n",
      ",    O\r\n",
      "however    O\r\n",
      ",    O\r\n",
      "the    O\r\n",
      "party    O\r\n",
      "did    O\r\n",
      "not    O\r\n",
      "join    O\r\n",
      "its    O\r\n",
      "Warsaw    B-Alliance114418822\r\n",
      "Pact    I-Alliance114418822\r\n",
      "brethren    O\r\n",
      "in    O\r\n",
      "de    O\r\n",
      "-    O\r\n",
      "Stalinization    O\r\n",
      ".    O\r\n",
      "\r\n",
      "On    O\r\n",
      "the    O\r\n",
      "outside    O\r\n",
      "too    O\r\n",
      ",    O\r\n",
      "the    O\r\n",
      "PMR    O\r\n",
      ",    O\r\n",
      "leading    O\r\n",
      "a    O\r\n",
      "country    O\r\n",
      "that    O\r\n",
      "had    O\r\n",
      "joined    O\r\n",
      "the    O\r\n",
      "Warsaw    B-Alliance114418822\r\n",
      "Pact    I-Alliance114418822\r\n",
      ",    O\r\n"
     ]
    }
   ],
   "source": [
    "# first 10 lines in test file\n",
    "! head -40 ../data/conll_test_files/Alliance114418822/Alliance114418822_fold1.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "# import utils\n",
    "import collections\n",
    "import codecs\n",
    "# import utils_nlp\n",
    "import re\n",
    "import time\n",
    "import token\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CoNLL_Parser(filepath):\n",
    "    \"\"\"\n",
    "    Need CoNLL file using -DOCSTART- or empty line for sentence sparator\n",
    "    \"\"\"\n",
    "    count_token = collections.Counter()\n",
    "    count_label = collections.Counter()\n",
    "    count_character = collections.Counter()\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    sentence_tokens = []\n",
    "    sentence_labels = []\n",
    "    if filepath:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().replace('    ', '\\t').split('\\t')\n",
    "                \n",
    "                # Save previous sentence at the end of it and start to the next sentence\n",
    "                if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "                    if len(sentence_tokens) > 0:\n",
    "                        labels.append(sentence_labels)\n",
    "                        tokens.append(sentence_tokens)\n",
    "                        sentence_tokens = []\n",
    "                        sentence_labels = []\n",
    "                    continue\n",
    "                \n",
    "                token = str(line[0])\n",
    "                label = str(line[-1])\n",
    "                count_token.update({token: 1})\n",
    "                count_label.update({label: 1})               \n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(label)\n",
    "                for character in token:\n",
    "                    count_character.update({character: 1})\n",
    "\n",
    "            if len(sentence_tokens) > 0:\n",
    "                labels.append(sentence_labels)\n",
    "                tokens.append(sentence_tokens)\n",
    "\n",
    "    return labels, tokens, count_token, count_label, count_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, tokens, token_count, label_count, character_count = CoNLL_Parser(path_test_conll_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Alliance114418822',\n",
       " 'I-Alliance114418822',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'the',\n",
       " 'party',\n",
       " 'did',\n",
       " 'not',\n",
       " 'join',\n",
       " 'its',\n",
       " 'Warsaw',\n",
       " 'Pact',\n",
       " 'brethren',\n",
       " 'in',\n",
       " 'de',\n",
       " '-',\n",
       " 'Stalinization',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of chars vocabu : 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('&', 5),\n",
       " ('’', 5),\n",
       " ('ö', 4),\n",
       " ('…', 4),\n",
       " ('ū', 4),\n",
       " ('è', 4),\n",
       " ('ō', 4),\n",
       " ('ć', 3),\n",
       " ('Ş', 3),\n",
       " ('ș', 3),\n",
       " ('ı', 3),\n",
       " ('в', 2),\n",
       " ('ë', 2),\n",
       " ('ł', 2),\n",
       " ('ā', 2),\n",
       " ('[', 2),\n",
       " ('ä', 2),\n",
       " (']', 2),\n",
       " ('ç', 2),\n",
       " ('ó', 2)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('size of chars vocabu :',len(character_count.keys()))\n",
    "character_count.most_common(100)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem the size of bigger than excepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Token2idx + Deal with UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_to_index = {}\n",
    "token_to_index[self.UNK] = self.UNK_TOKEN_INDEX\n",
    "iteration_number = 0\n",
    "number_of_unknown_tokens = 0\n",
    "\n",
    "for token, count in token_count['all'].items():\n",
    "    if iteration_number == self.UNK_TOKEN_INDEX: iteration_number += 1\n",
    "\n",
    "    if parameters['remap_unknown_tokens_to_unk'] == 1 and \\\n",
    "        (token_count['train'][token] == 0 or \\\n",
    "        parameters['load_only_pretrained_token_embeddings']) and \\\n",
    "        not utils_nlp.is_token_in_pretrained_embeddings(token, token_to_vector, parameters) and \\\n",
    "        token not in all_tokens_in_pretraining_dataset:\n",
    "        token_to_index[token] =  self.UNK_TOKEN_INDEX\n",
    "        number_of_unknown_tokens += 1\n",
    "        self.tokens_mapped_to_unk.append(token)\n",
    "    else:\n",
    "        token_to_index[token] = iteration_number\n",
    "        iteration_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
