{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Test create Vocabulary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "# import utils\n",
    "import collections\n",
    "import codecs\n",
    "# import utils_nlp\n",
    "import re\n",
    "import time\n",
    "import token\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "path_train = '../data/CoNLL2003/eng.train'\n",
    "path_eval = '../data/CoNLL2003/eng.testa'\n",
    "path_test = '../data/CoNLL2003/eng.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- O O\r\n",
      "\r\n",
      "EU NNP I-NP I-ORG\r\n",
      "rejects VBZ I-VP O\r\n",
      "German JJ I-NP I-MISC\r\n",
      "call NN I-NP O\r\n",
      "to TO I-VP O\r\n",
      "boycott VB I-VP O\r\n",
      "British JJ I-NP I-MISC\r\n",
      "lamb NN I-NP O\r\n"
     ]
    }
   ],
   "source": [
    "# first 10 lines in test file\n",
    "! head -10 ../data/CoNLL2003/eng.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_vocab(filename):\n",
    "    vocab = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_2idx(vocabu, save_idx = False, file_path = None):\n",
    "    dictionary = dict()\n",
    "    for idx, word in enumerate(vocabu):\n",
    "        word = word.strip()\n",
    "        dictionary[word] = idx\n",
    "        \n",
    "    # save index    \n",
    "    if save_idx:\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(file_path))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise                    \n",
    "        with open(file_path, 'w+') as fp:\n",
    "            json.dump(dictionary, fp, indent=4)\n",
    "            \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_lookup_table(vocab, glove_filename, dim = 100, save_table = False, file_path = None):\n",
    "\n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    with open(glove_filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "                \n",
    "    # save lookup table\n",
    "    if save_table:\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(file_path))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise                    \n",
    "        with open(file_path, 'w+') as fp:\n",
    "            np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabs(filepath, sparator = ' ', lowercase = True):\n",
    "    \n",
    "    count_token = collections.Counter()\n",
    "    count_label = collections.Counter()\n",
    "    count_character = collections.Counter()\n",
    "    \n",
    "    if filepath:\n",
    "        f = codecs.open(filepath, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            line = line.strip().split(sparator)\n",
    "\n",
    "            #skip sentence separator\n",
    "            if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "                continue\n",
    "            \n",
    "            token = str(line[0])\n",
    "            for character in token:\n",
    "                count_character.update({character: 1})\n",
    "                \n",
    "            # lowercase & digit\n",
    "            if lowercase:\n",
    "                token = str(line[0]).lower()\n",
    "            else:\n",
    "                token = str(line[0])\n",
    "                \n",
    "            # use the digit in pretrained embedding\n",
    "#             if token.isdigit():\n",
    "#                 token = '$NUM$'\n",
    "                \n",
    "            label = str(line[-1])\n",
    "            count_token.update({token: 1})\n",
    "            count_label.update({label: 1})              \n",
    "        \n",
    "        f.close()    \n",
    "            \n",
    "    return count_token, count_label, count_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "vocabulary for this corpus: 26869 tokens, 85 labels, 8 chars\n",
      "final vocabulary : 22948 vocabs\n",
      "vocabulary construction time:  19.17466149595566\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "print(\"Building vocab...\")\n",
    "count_token = {} \n",
    "count_label = {} \n",
    "count_character = {}\n",
    "\n",
    "datasets = [('train',path_train), ('eval', path_eval), ('test', path_test)]\n",
    "for dataset in datasets:\n",
    "    count_token[dataset[0]], count_label[dataset[0]], count_character[dataset[0]] = get_vocabs(dataset[1])\n",
    "\n",
    "vocab_token_corpus = count_token['train'] + count_token['eval'] + count_token['test']\n",
    "vocab_label = count_label['train'] + count_label['eval'] + count_label['test']\n",
    "vocab_char = count_character['train'] + count_character['eval'] + count_character['test']\n",
    "\n",
    "# sorted the vocabu by frequency \n",
    "vocab_token_corpus = [x[0] for x in vocab_token_corpus.most_common()]\n",
    "vocab_label = [x[0] for x in vocab_label.most_common()]\n",
    "vocab_char = [x[0] for x in vocab_char.most_common()]\n",
    "\n",
    "# future features: limit the vocabulary by threshold\n",
    "###############################################\n",
    "# if config.vocabulary_threshold > 1:\n",
    "#     vocab_token_corpus = \n",
    "###############################################\n",
    "\n",
    "# vocab in pre-trained embedding\n",
    "filename_glove = '../data/glove/glove.6B.100d.txt'\n",
    "vocab_glove = get_glove_vocab(filename_glove)\n",
    "\n",
    "# selected only common vocabs in corpus and pre-trained embedding(like glove)\n",
    "vocab_token_final = [token for token in vocab_token_corpus if token.strip() in vocab_glove]\n",
    "vocab_token_final = ['$UNK$'] + vocab_token_final\n",
    "\n",
    "\n",
    "# generate 2idx mapping dict for token, char, label\n",
    "path = '../data/idx/'\n",
    "save_idx = True\n",
    "paths = ['../data/idx/token2idx.json', '../data/idx/label2idx.json', '../data/idx/tag2idx.json']\n",
    "token2idx = get_2idx(vocab_token_final, save_idx, paths[0])\n",
    "char2idx = get_2idx(vocab_char, save_idx, paths[1])\n",
    "label2idx = get_2idx(vocab_label, save_idx, paths[2])\n",
    "\n",
    "# get embedding lookup table\n",
    "lookup_table = get_embedding_lookup_table(token2idx, filename_glove)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"vocabulary for this corpus: {} tokens, {} labels, {} chars\"\n",
    "      .format(len(vocab_token_corpus), len(vocab_char),len(vocab_label)))\n",
    "print(\"final vocabulary : {} vocabs\"\n",
    "      .format(len(vocab_token_final)))\n",
    "print('vocabulary construction time: ', stop - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 check internal variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$UNK$': 0,\n",
       " 'the': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'to': 6,\n",
       " 'a': 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " 'and': 10,\n",
       " '\"': 11,\n",
       " 'on': 12,\n",
       " 'said': 13,\n",
       " \"'s\": 14,\n",
       " 'for': 15,\n",
       " '-': 16,\n",
       " '1': 17,\n",
       " 'at': 18,\n",
       " 'was': 19,\n",
       " '2': 20,\n",
       " 'with': 21,\n",
       " '3': 22,\n",
       " '0': 23,\n",
       " 'that': 24,\n",
       " 'he': 25,\n",
       " 'from': 26,\n",
       " 'by': 27,\n",
       " 'it': 28,\n",
       " ':': 29,\n",
       " 'is': 30,\n",
       " '4': 31,\n",
       " 'as': 32,\n",
       " 'his': 33,\n",
       " 'had': 34,\n",
       " 'were': 35,\n",
       " 'an': 36,\n",
       " 'but': 37,\n",
       " 'not': 38,\n",
       " 'after': 39,\n",
       " 'has': 40,\n",
       " 'be': 41,\n",
       " 'have': 42,\n",
       " 'new': 43,\n",
       " 'first': 44,\n",
       " 'who': 45,\n",
       " '5': 46,\n",
       " 'will': 47,\n",
       " '6': 48,\n",
       " 'two': 49,\n",
       " 'they': 50,\n",
       " 'u.s.': 51,\n",
       " '$': 52,\n",
       " 'been': 53,\n",
       " 'their': 54,\n",
       " 'i': 55,\n",
       " 'are': 56,\n",
       " 'which': 57,\n",
       " 'would': 58,\n",
       " '--': 59,\n",
       " 'beat': 60,\n",
       " 'friday': 61,\n",
       " 'this': 62,\n",
       " '7': 63,\n",
       " 'up': 64,\n",
       " 'its': 65,\n",
       " 'percent': 66,\n",
       " 'one': 67,\n",
       " 'out': 68,\n",
       " 'we': 69,\n",
       " 'year': 70,\n",
       " 'thursday': 71,\n",
       " 'over': 72,\n",
       " 'last': 73,\n",
       " 'million': 74,\n",
       " 'government': 75,\n",
       " 'police': 76,\n",
       " 'against': 77,\n",
       " 'results': 78,\n",
       " '10': 79,\n",
       " 'world': 80,\n",
       " 'when': 81,\n",
       " 'second': 82,\n",
       " 'soccer': 83,\n",
       " '9': 84,\n",
       " 'saturday': 85,\n",
       " '/': 86,\n",
       " '8': 87,\n",
       " 'three': 88,\n",
       " 'also': 89,\n",
       " 'division': 90,\n",
       " 'wednesday': 91,\n",
       " 'more': 92,\n",
       " 'no': 93,\n",
       " 'told': 94,\n",
       " 'about': 95,\n",
       " 'president': 96,\n",
       " 'people': 97,\n",
       " 'league': 98,\n",
       " 'there': 99,\n",
       " 'london': 100,\n",
       " 'group': 101,\n",
       " 'or': 102,\n",
       " 'cup': 103,\n",
       " 'york': 104,\n",
       " '15': 105,\n",
       " 'some': 106,\n",
       " 'into': 107,\n",
       " 'under': 108,\n",
       " 'week': 109,\n",
       " 'germany': 110,\n",
       " 'minister': 111,\n",
       " 'tuesday': 112,\n",
       " 'won': 113,\n",
       " 'market': 114,\n",
       " 'all': 115,\n",
       " 'south': 116,\n",
       " 'match': 117,\n",
       " \"'\": 118,\n",
       " '11': 119,\n",
       " 'australia': 120,\n",
       " 'than': 121,\n",
       " 'before': 122,\n",
       " 'monday': 123,\n",
       " '13': 124,\n",
       " 'state': 125,\n",
       " 'played': 126,\n",
       " 'france': 127,\n",
       " 'time': 128,\n",
       " 'between': 129,\n",
       " 'her': 130,\n",
       " 'points': 131,\n",
       " 'other': 132,\n",
       " 'could': 133,\n",
       " 'city': 134,\n",
       " 'england': 135,\n",
       " 'if': 136,\n",
       " 'only': 137,\n",
       " '20': 138,\n",
       " 'since': 139,\n",
       " '12': 140,\n",
       " '14': 141,\n",
       " 'national': 142,\n",
       " 'years': 143,\n",
       " '21': 144,\n",
       " 'round': 145,\n",
       " 'games': 146,\n",
       " 'officials': 147,\n",
       " 'third': 148,\n",
       " 'four': 149,\n",
       " 'united': 150,\n",
       " '22': 151,\n",
       " 'sunday': 152,\n",
       " 'party': 153,\n",
       " 'off': 154,\n",
       " 'bank': 155,\n",
       " 'next': 156,\n",
       " 'home': 157,\n",
       " 'day': 158,\n",
       " 'she': 159,\n",
       " 'six': 160,\n",
       " 'win': 161,\n",
       " 'down': 162,\n",
       " 'five': 163,\n",
       " 'him': 164,\n",
       " 'open': 165,\n",
       " 'former': 166,\n",
       " 'company': 167,\n",
       " 'newsroom': 168,\n",
       " 'may': 169,\n",
       " '16': 170,\n",
       " 'did': 171,\n",
       " 'britain': 172,\n",
       " 'russia': 173,\n",
       " 'standings': 174,\n",
       " 'china': 175,\n",
       " 'foreign': 176,\n",
       " 'international': 177,\n",
       " 'chicago': 178,\n",
       " 'italy': 179,\n",
       " 'lost': 180,\n",
       " 'spokesman': 181,\n",
       " 'v': 182,\n",
       " 'july': 183,\n",
       " 'because': 184,\n",
       " 'men': 185,\n",
       " 'news': 186,\n",
       " 'august': 187,\n",
       " 'women': 188,\n",
       " 'made': 189,\n",
       " 'official': 190,\n",
       " \"n't\": 191,\n",
       " 'b': 192,\n",
       " 'any': 193,\n",
       " 'back': 194,\n",
       " 'do': 195,\n",
       " 'while': 196,\n",
       " 'spain': 197,\n",
       " '70': 198,\n",
       " 'just': 199,\n",
       " '1996': 200,\n",
       " 'september': 201,\n",
       " 'european': 202,\n",
       " 'them': 203,\n",
       " 'expected': 204,\n",
       " 'german': 205,\n",
       " 'where': 206,\n",
       " 'peace': 207,\n",
       " '30': 208,\n",
       " 'japan': 209,\n",
       " 'team': 210,\n",
       " 'through': 211,\n",
       " 'west': 212,\n",
       " '3.': 213,\n",
       " '6-4': 214,\n",
       " '1.': 215,\n",
       " '2.': 216,\n",
       " '17': 217,\n",
       " 'matches': 218,\n",
       " 'statement': 219,\n",
       " 'meeting': 220,\n",
       " 'being': 221,\n",
       " 'talks': 222,\n",
       " 'during': 223,\n",
       " '6-3': 224,\n",
       " '1/2': 225,\n",
       " 'british': 226,\n",
       " 'now': 227,\n",
       " ';': 228,\n",
       " 'shares': 229,\n",
       " 'russian': 230,\n",
       " '69': 231,\n",
       " 'game': 232,\n",
       " 'june': 233,\n",
       " 'most': 234,\n",
       " 'pakistan': 235,\n",
       " '25': 236,\n",
       " 'central': 237,\n",
       " '71': 238,\n",
       " 'killed': 239,\n",
       " 'lead': 240,\n",
       " 'added': 241,\n",
       " 'final': 242,\n",
       " 'earlier': 243,\n",
       " 'take': 244,\n",
       " 'set': 245,\n",
       " 'reported': 246,\n",
       " 'clinton': 247,\n",
       " 'billion': 248,\n",
       " 'month': 249,\n",
       " 'end': 250,\n",
       " 'reuters': 251,\n",
       " '6-2': 252,\n",
       " 'american': 253,\n",
       " 'prime': 254,\n",
       " '24': 255,\n",
       " 'war': 256,\n",
       " 'our': 257,\n",
       " 'trade': 258,\n",
       " 'metres': 259,\n",
       " 'close': 260,\n",
       " 'st': 261,\n",
       " 'saying': 262,\n",
       " 'seconds': 263,\n",
       " 'took': 264,\n",
       " 'major': 265,\n",
       " 'still': 266,\n",
       " 'leading': 267,\n",
       " 'minutes': 268,\n",
       " 'states': 269,\n",
       " 'season': 270,\n",
       " '18': 271,\n",
       " 'security': 272,\n",
       " 'half': 273,\n",
       " 'french': 274,\n",
       " 'political': 275,\n",
       " 'moscow': 276,\n",
       " 'cricket': 277,\n",
       " 'san': 278,\n",
       " 'israel': 279,\n",
       " 'capital': 280,\n",
       " 'union': 281,\n",
       " 'sweden': 282,\n",
       " 'seven': 283,\n",
       " 'innings': 284,\n",
       " 'should': 285,\n",
       " 'held': 286,\n",
       " 'play': 287,\n",
       " 'tonnes': 288,\n",
       " 'ministry': 289,\n",
       " 'pct': 290,\n",
       " 'early': 291,\n",
       " 'northern': 292,\n",
       " 'days': 293,\n",
       " 'part': 294,\n",
       " 'mark': 295,\n",
       " 'due': 296,\n",
       " 'so': 297,\n",
       " 'championship': 298,\n",
       " '5.': 299,\n",
       " 'country': 300,\n",
       " 'net': 301,\n",
       " 'both': 302,\n",
       " 'around': 303,\n",
       " 'then': 304,\n",
       " 'minute': 305,\n",
       " 'court': 306,\n",
       " 'left': 307,\n",
       " 'lower': 308,\n",
       " 'sales': 309,\n",
       " 'victory': 310,\n",
       " 'can': 311,\n",
       " 'what': 312,\n",
       " '100': 313,\n",
       " 'town': 314,\n",
       " 'prices': 315,\n",
       " 'run': 316,\n",
       " 'deal': 317,\n",
       " 'ago': 318,\n",
       " 'general': 319,\n",
       " 'price': 320,\n",
       " 'iraq': 321,\n",
       " 'republic': 322,\n",
       " '72': 323,\n",
       " 'result': 324,\n",
       " 'tennis': 325,\n",
       " 'you': 326,\n",
       " 'india': 327,\n",
       " 'hong': 328,\n",
       " 'number': 329,\n",
       " 'north': 330,\n",
       " 'per': 331,\n",
       " 'well': 332,\n",
       " 'czech': 333,\n",
       " 'netherlands': 334,\n",
       " 'belgium': 335,\n",
       " 'total': 336,\n",
       " 'eight': 337,\n",
       " 'goals': 338,\n",
       " '28': 339,\n",
       " '23': 340,\n",
       " '68': 341,\n",
       " 'agency': 342,\n",
       " 'elections': 343,\n",
       " 'washington': 344,\n",
       " 'east': 345,\n",
       " 'english': 346,\n",
       " 'very': 347,\n",
       " 'iraqi': 348,\n",
       " 'military': 349,\n",
       " '75': 350,\n",
       " 'halftime': 351,\n",
       " 'my': 352,\n",
       " 'cents': 353,\n",
       " 'says': 354,\n",
       " 'called': 355,\n",
       " 'leader': 356,\n",
       " 'dutch': 357,\n",
       " 'africa': 358,\n",
       " 'kong': 359,\n",
       " 'found': 360,\n",
       " 'make': 361,\n",
       " '1995': 362,\n",
       " '67': 363,\n",
       " 'man': 364,\n",
       " 'visit': 365,\n",
       " 'local': 366,\n",
       " 'rate': 367,\n",
       " 'put': 368,\n",
       " 'say': 369,\n",
       " '50': 370,\n",
       " 'series': 371,\n",
       " 'c': 372,\n",
       " '26': 373,\n",
       " 'meet': 374,\n",
       " 'months': 375,\n",
       " 'same': 376,\n",
       " '19': 377,\n",
       " 'good': 378,\n",
       " 'go': 379,\n",
       " 'newspaper': 380,\n",
       " 'another': 381,\n",
       " 'champion': 382,\n",
       " 'issue': 383,\n",
       " 'late': 384,\n",
       " 'top': 385,\n",
       " 'place': 386,\n",
       " 'start': 387,\n",
       " 'michael': 388,\n",
       " 'austria': 389,\n",
       " 'canada': 390,\n",
       " 'runs': 391,\n",
       " 'tour': 392,\n",
       " 'gave': 393,\n",
       " 'office': 394,\n",
       " 'following': 395,\n",
       " 'rebels': 396,\n",
       " 'troops': 397,\n",
       " 'dollar': 398,\n",
       " 'ended': 399,\n",
       " 'paris': 400,\n",
       " 'players': 401,\n",
       " 'profit': 402,\n",
       " 'israeli': 403,\n",
       " 'test': 404,\n",
       " 'record': 405,\n",
       " 'stock': 406,\n",
       " 'de': 407,\n",
       " '73': 408,\n",
       " '6-1': 409,\n",
       " 'conference': 410,\n",
       " 'until': 411,\n",
       " 'hit': 412,\n",
       " 'many': 413,\n",
       " '7-6': 414,\n",
       " '29': 415,\n",
       " 'african': 416,\n",
       " 'opposition': 417,\n",
       " '31': 418,\n",
       " 'behind': 419,\n",
       " 'attendance': 420,\n",
       " 'race': 421,\n",
       " 'think': 422,\n",
       " 'commission': 423,\n",
       " '27': 424,\n",
       " 'leaders': 425,\n",
       " 'forces': 426,\n",
       " 'press': 427,\n",
       " 'economic': 428,\n",
       " 'tournament': 429,\n",
       " 'western': 430,\n",
       " 'support': 431,\n",
       " 'report': 432,\n",
       " 'quoted': 433,\n",
       " 'david': 434,\n",
       " 'house': 435,\n",
       " 'australian': 436,\n",
       " 'parliament': 437,\n",
       " 'these': 438,\n",
       " '66': 439,\n",
       " 'singles': 440,\n",
       " 'several': 441,\n",
       " 'authorities': 442,\n",
       " 'closed': 443,\n",
       " '74': 444,\n",
       " 'head': 445,\n",
       " 'miles': 446,\n",
       " 'including': 447,\n",
       " 'area': 448,\n",
       " 'van': 449,\n",
       " 'going': 450,\n",
       " 'later': 451,\n",
       " 'date': 452,\n",
       " 'inc': 453,\n",
       " '64': 454,\n",
       " '65': 455,\n",
       " 'refugees': 456,\n",
       " 'fell': 457,\n",
       " 'side': 458,\n",
       " '60': 459,\n",
       " 'yeltsin': 460,\n",
       " 'chief': 461,\n",
       " 'john': 462,\n",
       " 'rose': 463,\n",
       " 'want': 464,\n",
       " 'came': 465,\n",
       " 'paul': 466,\n",
       " 'get': 467,\n",
       " 'jerusalem': 468,\n",
       " 'democratic': 469,\n",
       " 'asked': 470,\n",
       " 'u.n.': 471,\n",
       " '7-5': 472,\n",
       " 'brazil': 473,\n",
       " 'drawn': 474,\n",
       " 'army': 475,\n",
       " 'agreement': 476,\n",
       " 'arrested': 477,\n",
       " 'trading': 478,\n",
       " 'like': 479,\n",
       " 'those': 480,\n",
       " 'weekend': 481,\n",
       " 'near': 482,\n",
       " 'ahead': 483,\n",
       " '1-0': 484,\n",
       " 'draw': 485,\n",
       " 'allowed': 486,\n",
       " 'high': 487,\n",
       " 'right': 488,\n",
       " 'region': 489,\n",
       " 'palestinian': 490,\n",
       " 'election': 491,\n",
       " 'amsterdam': 492,\n",
       " 'km': 493,\n",
       " 'club': 494,\n",
       " 'best': 495,\n",
       " 'zealand': 496,\n",
       " 'further': 497,\n",
       " '...': 498,\n",
       " 'demand': 499,\n",
       " 'air': 500,\n",
       " 'taking': 501,\n",
       " 'atlanta': 502,\n",
       " 'plan': 503,\n",
       " 'give': 504,\n",
       " 'own': 505,\n",
       " 'ireland': 506,\n",
       " 'eastern': 507,\n",
       " '40': 508,\n",
       " 'grand': 509,\n",
       " 'already': 510,\n",
       " 'bond': 511,\n",
       " 'return': 512,\n",
       " 'arafat': 513,\n",
       " 'markets': 514,\n",
       " 'rights': 515,\n",
       " 'sri': 516,\n",
       " 'money': 517,\n",
       " 'index': 518,\n",
       " 'work': 519,\n",
       " 'southern': 520,\n",
       " 'higher': 521,\n",
       " 'business': 522,\n",
       " 'oil': 523,\n",
       " 'gmt': 524,\n",
       " 'away': 525,\n",
       " 'm.': 526,\n",
       " 'qualifier': 527,\n",
       " 'december': 528,\n",
       " 'captain': 529,\n",
       " 'power': 530,\n",
       " 'nations': 531,\n",
       " 'lebed': 532,\n",
       " 'gold': 533,\n",
       " 'bonds': 534,\n",
       " 'such': 535,\n",
       " 'toronto': 536,\n",
       " 'law': 537,\n",
       " 'me': 538,\n",
       " 'loss': 539,\n",
       " 'centre': 540,\n",
       " 'vs.': 541,\n",
       " 'plans': 542,\n",
       " 'exchange': 543,\n",
       " 'agreed': 544,\n",
       " 'long': 545,\n",
       " 'service': 546,\n",
       " 'campaign': 547,\n",
       " 'fighting': 548,\n",
       " 'california': 549,\n",
       " 'way': 550,\n",
       " 'white': 551,\n",
       " '1994': 552,\n",
       " 'shot': 553,\n",
       " 'overs': 554,\n",
       " 'decision': 555,\n",
       " 'much': 556,\n",
       " 'italian': 557,\n",
       " 'us': 558,\n",
       " 'poland': 559,\n",
       " 'control': 560,\n",
       " 'strong': 561,\n",
       " '1997': 562,\n",
       " 'weeks': 563,\n",
       " 'title': 564,\n",
       " 'indonesia': 565,\n",
       " 'baseball': 566,\n",
       " 'chechnya': 567,\n",
       " 'taken': 568,\n",
       " 'march': 569,\n",
       " 'hours': 570,\n",
       " 'reporters': 571,\n",
       " 'announced': 572,\n",
       " 'without': 573,\n",
       " 'night': 574,\n",
       " 'scores': 575,\n",
       " 'l': 576,\n",
       " 'louis': 577,\n",
       " 'nine': 578,\n",
       " 'airport': 579,\n",
       " 'seen': 580,\n",
       " 'whether': 581,\n",
       " 'corp': 582,\n",
       " 'past': 583,\n",
       " 'ahmed': 584,\n",
       " 'wickets': 585,\n",
       " 'hospital': 586,\n",
       " 'colorado': 587,\n",
       " 'quarter': 588,\n",
       " 'previous': 589,\n",
       " 'attack': 590,\n",
       " 'cash': 591,\n",
       " 'interest': 592,\n",
       " 'squad': 593,\n",
       " 'winning': 594,\n",
       " 'los': 595,\n",
       " 'order': 596,\n",
       " 'television': 597,\n",
       " 'manager': 598,\n",
       " 'current': 599,\n",
       " 'financial': 600,\n",
       " 'began': 601,\n",
       " 'future': 602,\n",
       " 'share': 603,\n",
       " 'martin': 604,\n",
       " 'scored': 605,\n",
       " '2-0': 606,\n",
       " 'coach': 607,\n",
       " 'angeles': 608,\n",
       " 'death': 609,\n",
       " 'dole': 610,\n",
       " 'died': 611,\n",
       " 'hold': 612,\n",
       " 'few': 613,\n",
       " 'members': 614,\n",
       " 'little': 615,\n",
       " 'mexico': 616,\n",
       " 'department': 617,\n",
       " 'europe': 618,\n",
       " 'public': 619,\n",
       " 'despite': 620,\n",
       " 'rates': 621,\n",
       " 'county': 622,\n",
       " 'fall': 623,\n",
       " 'sydney': 624,\n",
       " 'boston': 625,\n",
       " 'went': 626,\n",
       " 'children': 627,\n",
       " 'come': 628,\n",
       " 'countries': 629,\n",
       " 'led': 630,\n",
       " 'sent': 631,\n",
       " 'main': 632,\n",
       " 'does': 633,\n",
       " 'analysts': 634,\n",
       " 'champions': 635,\n",
       " 'seattle': 636,\n",
       " 'released': 637,\n",
       " 'w': 638,\n",
       " 'old': 639,\n",
       " 'workers': 640,\n",
       " 'madrid': 641,\n",
       " 'plane': 642,\n",
       " 'fourth': 643,\n",
       " 'baltimore': 644,\n",
       " 'texas': 645,\n",
       " 'philadelphia': 646,\n",
       " 'hits': 647,\n",
       " 'co': 648,\n",
       " 'forecast': 649,\n",
       " 'real': 650,\n",
       " 'yen': 651,\n",
       " 'radio': 652,\n",
       " 'opening': 653,\n",
       " 'growth': 654,\n",
       " 'kurdish': 655,\n",
       " 'finland': 656,\n",
       " 'romania': 657,\n",
       " 'signed': 658,\n",
       " 'lanka': 659,\n",
       " 'latest': 660,\n",
       " 'production': 661,\n",
       " 'council': 662,\n",
       " 'rugby': 663,\n",
       " 'ban': 664,\n",
       " 'human': 665,\n",
       " 'florida': 666,\n",
       " 'daily': 667,\n",
       " 'details': 668,\n",
       " 'period': 669,\n",
       " 'pay': 670,\n",
       " 'today': 671,\n",
       " 'help': 672,\n",
       " 'average': 673,\n",
       " 'data': 674,\n",
       " 'strike': 675,\n",
       " 'april': 676,\n",
       " '63': 677,\n",
       " 'switzerland': 678,\n",
       " 'francisco': 679,\n",
       " 'olympic': 680,\n",
       " 'tabulate': 681,\n",
       " 'detroit': 682,\n",
       " 'morning': 683,\n",
       " 'case': 684,\n",
       " 'traders': 685,\n",
       " 'singapore': 686,\n",
       " 'conditions': 687,\n",
       " 'might': 688,\n",
       " 'available': 689,\n",
       " 'recent': 690,\n",
       " 'again': 691,\n",
       " 'call': 692,\n",
       " 'port': 693,\n",
       " 'possible': 694,\n",
       " 'jordan': 695,\n",
       " 'free': 696,\n",
       " 'october': 697,\n",
       " 'street': 698,\n",
       " 'scorers': 699,\n",
       " 'belgian': 700,\n",
       " 'yet': 701,\n",
       " 'moslem': 702,\n",
       " 'comment': 703,\n",
       " 'peter': 704,\n",
       " 'contract': 705,\n",
       " 'federal': 706,\n",
       " 'force': 707,\n",
       " 'polish': 708,\n",
       " 'turkey': 709,\n",
       " 'industry': 710,\n",
       " '96': 711,\n",
       " 'golf': 712,\n",
       " 'premier': 713,\n",
       " 'canadian': 714,\n",
       " 'vs': 715,\n",
       " 'milwaukee': 716,\n",
       " 'fifth': 717,\n",
       " 'each': 718,\n",
       " 'university': 719,\n",
       " 'bill': 720,\n",
       " 'prison': 721,\n",
       " 'coast': 722,\n",
       " 'students': 723,\n",
       " 'brussels': 724,\n",
       " 'reports': 725,\n",
       " 'accused': 726,\n",
       " 'must': 727,\n",
       " 'outside': 728,\n",
       " 'immediately': 729,\n",
       " 'given': 730,\n",
       " 'scotland': 731,\n",
       " 'minnesota': 732,\n",
       " 'got': 733,\n",
       " 'budget': 734,\n",
       " 'policy': 735,\n",
       " 'bid': 736,\n",
       " 'action': 737,\n",
       " 'known': 738,\n",
       " 'goal': 739,\n",
       " 'working': 740,\n",
       " 'failed': 741,\n",
       " 'likely': 742,\n",
       " 'too': 743,\n",
       " 'kansas': 744,\n",
       " '62': 745,\n",
       " 'houston': 746,\n",
       " 'making': 747,\n",
       " 'road': 748,\n",
       " 'even': 749,\n",
       " 'times': 750,\n",
       " 'point': 751,\n",
       " 'least': 752,\n",
       " 'small': 753,\n",
       " 'rise': 754,\n",
       " 'economy': 755,\n",
       " 'see': 756,\n",
       " 'used': 757,\n",
       " 'secretary': 758,\n",
       " 'aug': 759,\n",
       " 'term': 760,\n",
       " 'trying': 761,\n",
       " 'red': 762,\n",
       " 'forced': 763,\n",
       " '76': 764,\n",
       " 'started': 765,\n",
       " 'services': 766,\n",
       " '54': 767,\n",
       " 'great': 768,\n",
       " 'cut': 769,\n",
       " '32': 770,\n",
       " 'although': 771,\n",
       " 'barcelona': 772,\n",
       " 'settlement': 773,\n",
       " 'ukraine': 774,\n",
       " 'association': 775,\n",
       " 'labour': 776,\n",
       " 'estimated': 777,\n",
       " 'manchester': 778,\n",
       " 'costa': 779,\n",
       " 'within': 780,\n",
       " '61': 781,\n",
       " 'swiss': 782,\n",
       " 'injured': 783,\n",
       " 'confirmed': 784,\n",
       " 'move': 785,\n",
       " 'bonn': 786,\n",
       " 'nearly': 787,\n",
       " 'island': 788,\n",
       " 'straight': 789,\n",
       " 'tax': 790,\n",
       " 'keep': 791,\n",
       " 'use': 792,\n",
       " 's.': 793,\n",
       " '59': 794,\n",
       " 'wasim': 795,\n",
       " 'norway': 796,\n",
       " 'wimbledon': 797,\n",
       " 'diego': 798,\n",
       " 'charges': 799,\n",
       " 'treaty': 800,\n",
       " 'position': 801,\n",
       " '33': 802,\n",
       " 'kenya': 803,\n",
       " 'committee': 804,\n",
       " 'health': 805,\n",
       " 'january': 806,\n",
       " 'black': 807,\n",
       " 'face': 808,\n",
       " 'rebel': 809,\n",
       " 'unless': 810,\n",
       " 'a.': 811,\n",
       " 'akram': 812,\n",
       " 'scottish': 813,\n",
       " 'thomas': 814,\n",
       " 'penalty': 815,\n",
       " 'cleveland': 816,\n",
       " 'montreal': 817,\n",
       " 'defence': 818,\n",
       " 'robert': 819,\n",
       " 'woman': 820,\n",
       " 'stocks': 821,\n",
       " 'companies': 822,\n",
       " 'ltd': 823,\n",
       " 'nigeria': 824,\n",
       " 'senior': 825,\n",
       " 'chairman': 826,\n",
       " 'jones': 827,\n",
       " 'full': 828,\n",
       " 'received': 829,\n",
       " 'dealers': 830,\n",
       " 'planned': 831,\n",
       " 'firm': 832,\n",
       " 'wall': 833,\n",
       " 'winner': 834,\n",
       " 'jose': 835,\n",
       " 'note': 836,\n",
       " 'groups': 837,\n",
       " 'clear': 838,\n",
       " 'investors': 839,\n",
       " 'information': 840,\n",
       " 'low': 841,\n",
       " 'vote': 842,\n",
       " 'disease': 843,\n",
       " 'across': 844,\n",
       " 'brought': 845,\n",
       " 'embassy': 846,\n",
       " 'iran': 847,\n",
       " 'level': 848,\n",
       " 'ceasefire': 849,\n",
       " 'board': 850,\n",
       " 'exports': 851,\n",
       " '77': 852,\n",
       " 'never': 853,\n",
       " 'mushtaq': 854,\n",
       " '0-0': 855,\n",
       " 'cincinnati': 856,\n",
       " 'oakland': 857,\n",
       " 'homer': 858,\n",
       " '34': 859,\n",
       " '1,000': 860,\n",
       " 'violence': 861,\n",
       " 'declared': 862,\n",
       " 'playing': 863,\n",
       " \"'m\": 864,\n",
       " 'whose': 865,\n",
       " 'life': 866,\n",
       " 'protest': 867,\n",
       " 'how': 868,\n",
       " 'province': 869,\n",
       " 'yr': 870,\n",
       " 'short': 871,\n",
       " 'here': 872,\n",
       " 'based': 873,\n",
       " 'earnings': 874,\n",
       " 'according': 875,\n",
       " 'know': 876,\n",
       " 'better': 877,\n",
       " 'slovakia': 878,\n",
       " 'striker': 879,\n",
       " 'pittsburgh': 880,\n",
       " 'ajax': 881,\n",
       " 'milan': 882,\n",
       " '35': 883,\n",
       " 'post': 884,\n",
       " 'fire': 885,\n",
       " 'bomb': 886,\n",
       " \"'re\": 887,\n",
       " 'chinese': 888,\n",
       " 'coming': 889,\n",
       " 'attacks': 890,\n",
       " 'stories': 891,\n",
       " '45': 892,\n",
       " 'front': 893,\n",
       " '2-1': 894,\n",
       " 'prix': 895,\n",
       " 'director': 896,\n",
       " 'named': 897,\n",
       " 'zimbabwe': 898,\n",
       " 'johnson': 899,\n",
       " '46': 900,\n",
       " 'young': 901,\n",
       " 'career': 902,\n",
       " 'school': 903,\n",
       " 'civil': 904,\n",
       " 'village': 905,\n",
       " 'showed': 906,\n",
       " 'charged': 907,\n",
       " 'himself': 908,\n",
       " 'independence': 909,\n",
       " 'king': 910,\n",
       " 'wheat': 911,\n",
       " '48': 912,\n",
       " 'one-day': 913,\n",
       " 'relations': 914,\n",
       " 'sale': 915,\n",
       " 'source': 916,\n",
       " 'baghdad': 917,\n",
       " 'scheduled': 918,\n",
       " 'armed': 919,\n",
       " 'november': 920,\n",
       " 'offered': 921,\n",
       " 'course': 922,\n",
       " 'korea': 923,\n",
       " 'teams': 924,\n",
       " 'ian': 925,\n",
       " 'ground': 926,\n",
       " 'medical': 927,\n",
       " 'defeat': 928,\n",
       " 'soon': 929,\n",
       " '&': 930,\n",
       " '36': 931,\n",
       " 'bosnia': 932,\n",
       " 'bosnian': 933,\n",
       " 'passengers': 934,\n",
       " 'trip': 935,\n",
       " 'process': 936,\n",
       " 'reached': 937,\n",
       " 'dutroux': 938,\n",
       " 'figures': 939,\n",
       " 'negotiations': 940,\n",
       " 'movement': 941,\n",
       " '42': 942,\n",
       " '53': 943,\n",
       " 'aggregate': 944,\n",
       " 'products': 945,\n",
       " 'korean': 946,\n",
       " '1992': 947,\n",
       " 'turned': 948,\n",
       " 'far': 949,\n",
       " 'convention': 950,\n",
       " 'having': 951,\n",
       " 'need': 952,\n",
       " 'indian': 953,\n",
       " 'struck': 954,\n",
       " 'defender': 955,\n",
       " 'republican': 956,\n",
       " '6-0': 957,\n",
       " '---': 958,\n",
       " 'denied': 959,\n",
       " 'mother': 960,\n",
       " 'netanyahu': 961,\n",
       " 'continue': 962,\n",
       " 'guerrillas': 963,\n",
       " 'border': 964,\n",
       " 'spot': 965,\n",
       " 'flight': 966,\n",
       " 'problem': 967,\n",
       " 'decided': 968,\n",
       " 'opened': 969,\n",
       " 'khan': 970,\n",
       " 'waqar': 971,\n",
       " 'rangers': 972,\n",
       " 'midfielder': 973,\n",
       " 'ninth': 974,\n",
       " 'system': 975,\n",
       " '55': 976,\n",
       " 'really': 977,\n",
       " '1993': 978,\n",
       " '56': 979,\n",
       " 'illegal': 980,\n",
       " 'berlin': 981,\n",
       " 'osce': 982,\n",
       " 'banks': 983,\n",
       " 'declined': 984,\n",
       " 'japanese': 985,\n",
       " 'ball': 986,\n",
       " 'lot': 987,\n",
       " 'democracy': 988,\n",
       " 'nation': 989,\n",
       " 'believed': 990,\n",
       " 'light': 991,\n",
       " 'met': 992,\n",
       " 'areas': 993,\n",
       " 'turkish': 994,\n",
       " 'competition': 995,\n",
       " '81': 996,\n",
       " '1-1': 997,\n",
       " 'park': 998,\n",
       " 'holiday': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars that in test not in train: {'#'}\n"
     ]
    }
   ],
   "source": [
    "print('chars that in test not in train:', count_character['test'].keys() - count_character['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label vocabs: ['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "print('label vocabs:', vocab_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars vocabs in train:84, vs in total corpus:85\n"
     ]
    }
   ],
   "source": [
    "print('chars vocabs in train:{}, vs in total corpus:{}'.format(len(count_character['train'].keys()), \n",
    "                                                               len(vocab_char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token vocabs that in corpus: 26869\n",
      "token vocabs that in pretrained-embedding: 400000\n",
      "final token vocabs: 22948\n",
      "shape of lookup table: (22948, 100)\n"
     ]
    }
   ],
   "source": [
    "print('token vocabs that in corpus:',len(vocab_token_corpus))\n",
    "print('token vocabs that in pretrained-embedding:',len(vocab_glove))\n",
    "print('final token vocabs:',len(vocab_token_final))\n",
    "print('shape of lookup table:',lookup_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 check finally result\n",
    "* extract embedding directly from glove \n",
    "* extract embedding from lookup table by token2idx\n",
    "* compare finally result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove(file_path):\n",
    "    glove = dict()\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            glove[word] = embedding\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = ['i', 'went', 'to', 'paris', 'yesterday', '.']\n",
    "filename_glove = '../data/glove/glove.6B.100d.txt'\n",
    "glove = get_glove(filename_glove)\n",
    "voctors_from_glove = np.asarray([glove[token] for token in test])\n",
    "voctors_from_table = np.asarray([lookup_table[token2idx[token]] for token in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if the finall result correct: True\n"
     ]
    }
   ],
   "source": [
    "print('if the finall result correct:',np.all(voctors_from_glove == voctors_from_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split CoNLL columns into sentences + Mapping to Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(dataset_filepath, token2idx, char2idx, label2idx, sparator = ' ', lowercase = True):\n",
    "    \n",
    "    # collection per sentence\n",
    "    # format [[[char_idxs], word_idx], ...]\n",
    "    sentence_token = []\n",
    "    # format [[label], ...]\n",
    "    sentence_label = []\n",
    "    \n",
    "    # format [[sentence1_token], [sentence2_token], ...]\n",
    "    tokens = []\n",
    "    # format [[sentence1_label], [sentence2_label], ...]\n",
    "    labels = []\n",
    "\n",
    "    # go throught whole CoNLL file\n",
    "    f = codecs.open(dataset_filepath, 'r', 'UTF-8')\n",
    "    for line in f:\n",
    "        line = line.strip().split(sparator)\n",
    "        # encouter a new sentence\n",
    "        if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "            if len(sentence_token) > 0:\n",
    "                labels.append(sentence_label)\n",
    "                tokens.append(sentence_token)\n",
    "                sentence_label = []\n",
    "                sentence_token = []\n",
    "            continue\n",
    "                \n",
    "        token = str(line[0])\n",
    "        label = str(line[-1])    \n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = token.lower()\n",
    "        # don't use NUM\n",
    "#         if word.isdigit():\n",
    "#             word = NUM\n",
    "\n",
    "        # char idxs\n",
    "        char_idxs = []\n",
    "        for char in word:\n",
    "            if char in char2idx:\n",
    "                char_idxs += [char2idx[char]]  \n",
    "            else:\n",
    "                print(\"encounter UNK char:\", char)\n",
    "        \n",
    "        # word idx\n",
    "        if word in token2idx:\n",
    "            word_idx = token2idx[word]\n",
    "        else:\n",
    "            word_idx = token2idx['$UNK$']\n",
    "        \n",
    "        # label idx\n",
    "        if label in label2idx:\n",
    "            label_idx = label2idx[label]\n",
    "        else:\n",
    "            print(\"encounter UNK label:\", label)\n",
    "            \n",
    "        sentence_token.append((char_idxs, word_idx))\n",
    "        sentence_label.append(label_idx)\n",
    "\n",
    "    if len(sentence_token) > 0:\n",
    "        tokens.append(sentence_token)\n",
    "        labels.append(sentence_label)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 check finally result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- O O\r\n",
      "\r\n",
      "CRICKET NNP I-NP O\r\n",
      "- : O O\r\n",
      "LEICESTERSHIRE NNP I-NP I-ORG\r\n",
      "TAKE NNP I-NP O\r\n",
      "OVER IN I-PP O\r\n",
      "AT NNP I-NP O\r\n",
      "TOP NNP I-NP O\r\n",
      "AFTER NNP I-NP O\r\n",
      "INNINGS NNP I-NP O\r\n",
      "VICTORY NN I-NP O\r\n",
      ". . O O\r\n",
      "\r\n",
      "LONDON NNP I-NP I-LOC\r\n",
      "1996-08-30 CD I-NP O\r\n",
      "\r\n",
      "\r\n",
      "test123 CD I-NP O\r\n",
      "123 CD I-NP O\r\n",
      "test CD I-NP O\r\n",
      "I CD I-NP O\r\n",
      "went CD I-NP O\r\n",
      "to CD I-NP O\r\n",
      "Paris CD I-NP I-LOC\r\n",
      "yesterday CD I-NP O\r\n",
      ". CD I-NP O\r\n",
      "\r\n",
      "CRICKET NNP I-NP O\r\n",
      "- : O O\r\n",
      "LEICESTERSHIRE NNP I-NP I-ORG\r\n",
      "TAKE NNP I-NP O\r\n",
      "OVER IN I-PP O\r\n",
      "AT NNP I-NP O\r\n",
      "TOP NNP I-NP O\r\n",
      "AFTER NNP I-NP O\r\n",
      "INNINGS NNP I-NP O\r\n",
      "VICTORY NN I-NP O\r\n",
      ". . O O\r\n",
      "\r\n",
      "LONDON NNP I-NP I-LOC\r\n",
      "1996-08-30 CD I-NP O\r\n",
      "\r\n",
      "\r\n",
      "test123 CD I-NP O\r\n",
      "123 CD I-NP O\r\n",
      "test CD I-NP O\r\n",
      "I CD I-NP O\r\n",
      "went CD I-NP O\r\n",
      "to CD I-NP O\r\n",
      "Paris CD I-NP I-LOC\r\n",
      "yesterday CD I-NP O\r\n",
      ". CD I-NP O\r\n"
     ]
    }
   ],
   "source": [
    "! cat ../data/test_conll_small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentence: 6\n",
      "number of token of test sentence: 9\n"
     ]
    }
   ],
   "source": [
    "dataset_filepath = '../data/test_conll_small1.txt'\n",
    "tokens, labels = get_inputs(dataset_filepath, token2idx, char2idx, label2idx)\n",
    "print('number of sentence:', len(tokens))\n",
    "print('number of token of test sentence:', len(tokens[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tokens: (6,)\n",
      "shape of labels: (6,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of tokens:', np.asarray(tokens).shape)\n",
    "print('shape of labels:', np.asarray(labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([2, 0, 7, 2, 21, 29, 37], 0),\n",
       " ([21, 29, 37], 12655),\n",
       " ([2, 0, 7, 2], 404),\n",
       " ([4], 55),\n",
       " ([19, 0, 3, 2], 626),\n",
       " ([2, 5], 6),\n",
       " ([15, 1, 6, 4, 7], 400),\n",
       " ([18, 0, 7, 2, 0, 6, 9, 1, 18], 2529),\n",
       " ([17], 3)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token for checking: [12655, 404, 55, 626, 6, 400, 2529, 3]\n"
     ]
    }
   ],
   "source": [
    "checking = [ '123', 'test', 'i', 'went', 'to', 'paris', 'yesterday', '.']\n",
    "print('token for checking:', [token2idx[token] for token in checking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token for checking: [18, 0, 7, 2, 0, 6, 9, 1, 18, 17]\n"
     ]
    }
   ],
   "source": [
    "checking = [ 'y', 'e', 's', 't', 'e', 'r','d','a', 'y', '.']\n",
    "print('token for checking:', [char2idx[token] for token in checking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 3, 0, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token for checking: [0, 0, 0, 0, 0, 0, 3, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "checking = [ 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O']\n",
    "print('token for checking:', [label2idx[token] for token in checking])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. shuffle data + Minibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[([11, 6, 4, 11, 28, 0, 2], 277),\n",
       "  ([22], 16),\n",
       "  ([8, 0, 4, 11, 0, 7, 2, 0, 6, 7, 10, 4, 6, 0], 1907),\n",
       "  ([2, 1, 28, 0], 244),\n",
       "  ([5, 26, 0, 6], 72),\n",
       "  ([1, 2], 18),\n",
       "  ([2, 5, 15], 385),\n",
       "  ([1, 14, 2, 0, 6], 39),\n",
       "  ([4, 3, 3, 4, 3, 16, 7], 284),\n",
       "  ([26, 4, 11, 2, 5, 6, 18], 310),\n",
       "  ([17], 3)],\n",
       " [([8, 5, 3, 9, 5, 3], 100), ([21, 31, 31, 32, 22, 24, 45, 22, 37, 24], 0)],\n",
       " [([2, 0, 7, 2, 21, 29, 37], 0),\n",
       "  ([21, 29, 37], 12655),\n",
       "  ([2, 0, 7, 2], 404),\n",
       "  ([4], 55),\n",
       "  ([19, 0, 3, 2], 626),\n",
       "  ([2, 5], 6),\n",
       "  ([15, 1, 6, 4, 7], 400),\n",
       "  ([18, 0, 7, 2, 0, 6, 9, 1, 18], 2529),\n",
       "  ([17], 3)],\n",
       " [([11, 6, 4, 11, 28, 0, 2], 277),\n",
       "  ([22], 16),\n",
       "  ([8, 0, 4, 11, 0, 7, 2, 0, 6, 7, 10, 4, 6, 0], 1907),\n",
       "  ([2, 1, 28, 0], 244),\n",
       "  ([5, 26, 0, 6], 72),\n",
       "  ([1, 2], 18),\n",
       "  ([2, 5, 15], 385),\n",
       "  ([1, 14, 2, 0, 6], 39),\n",
       "  ([4, 3, 3, 4, 3, 16, 7], 284),\n",
       "  ([26, 4, 11, 2, 5, 6, 18], 310),\n",
       "  ([17], 3)],\n",
       " [([8, 5, 3, 9, 5, 3], 100), ([21, 31, 31, 32, 22, 24, 45, 22, 37, 24], 0)],\n",
       " [([2, 0, 7, 2, 21, 29, 37], 0),\n",
       "  ([21, 29, 37], 12655),\n",
       "  ([2, 0, 7, 2], 404),\n",
       "  ([4], 55),\n",
       "  ([19, 0, 3, 2], 626),\n",
       "  ([2, 5], 6),\n",
       "  ([15, 1, 6, 4, 7], 400),\n",
       "  ([18, 0, 7, 2, 0, 6, 9, 1, 18], 2529),\n",
       "  ([17], 3)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [3, 0],\n",
       " [0, 0, 0, 0, 0, 0, 3, 0, 0],\n",
       " [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [3, 0],\n",
       " [0, 0, 0, 0, 0, 0, 3, 0, 0]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(tokens, labels, batch_size = 1, shuffle = True):\n",
    "    # shuffle the data at the beginning of each epoch\n",
    "    data = np.array([[tokens[i], labels[i]] for i in range(len(tokens))])\n",
    "    np.random.shuffle(data)\n",
    "    token, labels = zip(*data)\n",
    "#     print(token)\n",
    "#     print()\n",
    "#     print(labels)\n",
    "    \n",
    "    # generate mini batches\n",
    "    for i in np.arange(0, len(token), batch_size):\n",
    "        offset = min(i+batch_size, len(token))\n",
    "        yield (token[i:offset], labels[i:offset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11\n",
      "11 11\n",
      "\n",
      "9 9\n",
      "9 9\n",
      "\n",
      "2 2\n",
      "2 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (x_batch, y_batch) in next_batch(tokens, labels,2):\n",
    "    print(len(x_batch[0]),len(x_batch[1]))\n",
    "    print(len(y_batch[0]),len(y_batch[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Padding sentence into fixed length sequence\n",
    "* reference https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (x_batch, y_batch) in next_batch(tokens, labels,6):\n",
    "    sentences = [list(zip(*x))[1] for x in x_batch]\n",
    "    char_sentences = [list(zip(*x))[0] for x in x_batch]\n",
    "    print(len(sentences))\n",
    "    print()\n",
    "    y_labels = y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 9\n",
      "11 9\n",
      "11 9\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences[0]), len(sentences[1]))\n",
    "print(len(char_sentences[0]), len(char_sentences[1]))\n",
    "print(len(y_labels[0]), len(y_labels[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence(batch_setence):\n",
    "    \n",
    "    # find the max_length\n",
    "    max_length = max(map(lambda x : len(x), batch_setence))\n",
    "    \n",
    "    # padding\n",
    "    sequence_padded = []\n",
    "    sequence_length = []\n",
    "    for seq in batch_setence:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [-1]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_word(batch_setence_word):\n",
    "    '''\n",
    "    https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html\n",
    "    '''\n",
    "    max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                           for seq in batch_setence_word])\n",
    "    sequence_padded, sequence_length = [], []\n",
    "    for seq in batch_setence_word:\n",
    "        # all words are same length now\n",
    "        sp, sl = _pad_sequences(seq, -1, max_length_word)\n",
    "        sequence_padded += [sp]\n",
    "        sequence_length += [sl]\n",
    "\n",
    "    max_length_sentence = max(map(lambda x : len(x), batch_setence_word))\n",
    "    sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "            [-1]*max_length_word, max_length_sentence)\n",
    "    sequence_length, _ = _pad_sequences(sequence_length, -1,\n",
    "            max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (x_batch, y_batch) in next_batch(tokens, labels,2):\n",
    "    sentences = [list(zip(*x))[1] for x in x_batch]\n",
    "    char_sentences = [list(zip(*x))[0] for x in x_batch]\n",
    "    y_labels = y_batch\n",
    "    pad_sentences,sentence_length = pad_sentence(sentences)\n",
    "    pad_char_sentences, word_length = pad_word(char_sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(277, 16, 1907, 244, 72, 18, 385, 39, 284, 310, 3), (100, 0)]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([11, 6, 4, 11, 28, 0, 2],\n",
       "  [22],\n",
       "  [8, 0, 4, 11, 0, 7, 2, 0, 6, 7, 10, 4, 6, 0],\n",
       "  [2, 1, 28, 0],\n",
       "  [5, 26, 0, 6],\n",
       "  [1, 2],\n",
       "  [2, 5, 15],\n",
       "  [1, 14, 2, 0, 6],\n",
       "  [4, 3, 3, 4, 3, 16, 7],\n",
       "  [26, 4, 11, 2, 5, 6, 18],\n",
       "  [17]),\n",
       " ([8, 5, 3, 9, 5, 3], [21, 31, 31, 32, 22, 24, 45, 22, 37, 24])]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[277, 16, 1907, 244, 72, 18, 385, 39, 284, 310, 3],\n",
       " [100, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 11]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2, 0, 7, 2, 21, 29, 37, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [21, 29, 37, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [2, 0, 7, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [19, 0, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [15, 1, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [18, 0, 7, 2, 0, 6, 9, 1, 18, -1, -1, -1, -1, -1],\n",
       "  [17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
       " [[11, 6, 4, 11, 28, 0, 2, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [22, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [8, 0, 4, 11, 0, 7, 2, 0, 6, 7, 10, 4, 6, 0],\n",
       "  [2, 1, 28, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [5, 26, 0, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [2, 5, 15, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [1, 14, 2, 0, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [4, 3, 3, 4, 3, 16, 7, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [26, 4, 11, 2, 5, 6, 18, -1, -1, -1, -1, -1, -1, -1],\n",
       "  [17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_char_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 3, 4, 1, 4, 2, 5, 9, 1, -1, -1], [7, 1, 14, 4, 4, 2, 3, 5, 7, 7, 1]]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* token_to_vector = utils_nlp.load_pretrained_token_embeddings(parameters) // do not modify like add new vectors\n",
    "\n",
    "* token_count['all'] : {word, count} : for all in train, test, valid, dev \n",
    "* add to token_count['all'] the vocabu in token_to_vector {-1}\n",
    "* same for chars\n",
    "* same for chars\n",
    "* order by frequency\n",
    "* set: token_to_index {word, integer} : \n",
    "    if not in train & not in token_to_vector & not in pretrained dataset --> UNK\n",
    "    == index ++\n",
    "* infrequent_token_indices\n",
    "\n",
    "\n",
    "* Label\n",
    "    * aligin ['B-', 'I-', 'E-', 'S-']\n",
    "    * order\n",
    "    * label_to_index\n",
    "\n",
    "* character\n",
    "    * character_to_index\n",
    "\n",
    "*idx2\n",
    "    index_to_token\n",
    "    label_to_index\n",
    "    index_to_character\n",
    "    \n",
    "    \n",
    "* _convert_to_indices\n",
    "    * 2idx\n",
    "    * max_length\n",
    "    * padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generators object\n",
    "dev   = CoNLLDataset(config.filename_dev, processing_word)\n",
    "    lowercase \n",
    "    digit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
