{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "# parameter define #######################################################\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128 ####\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# build computational graph###############################################\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    # list[tensor1,... tensor28], tensori.shape = (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.LSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "# logits means: tge non-normalized predictions for a classification model, \n",
    "# it need to be normalized by a softmax to get the probabilities for each possible class\n",
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements, batch_x become nparrary (batch_size, 28, 28)\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand:\n",
    " * change .rnn.BasicLSTMCell() --> .rnn.LSTMCell() work\n",
    " * tf.unstack()\n",
    " * a RNN network can be split into two parts:\n",
    "     * lstm_cell = rnn.LSTMCell(num_hidden, forget_bias=1.0): build the unrolled hidden state\n",
    "     * outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32): introduce the time step to roll up above cell\n",
    "\n",
    " * LSTMCell()\n",
    "     * num_units\n",
    "     * num_proj: output projection\n",
    "     * activation: for inner statesL tanh\n",
    " * static_rnn:\n",
    "     * output: list of time step size\n",
    "     * state: only final state\n",
    "\n",
    "### Question:\n",
    " * how RNN update, time step by time step, detail is skip by the code\n",
    " * what if sentence with different lenght\n",
    "     * static_rnn vs. dynamic_rnn\n",
    " * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * use stiac_rnn + sequence_length\n",
    " * may need function to etract the last output, since different lengths in batch\n",
    " * the other part can be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * use lstm_fw_cell + lstm_bw_cell + static_bidirectional_rnn\n",
    " * the other part can be the same\n",
    " * the ouptut format is [time step list:[tensor1], ...[tensor_last_time_slot]]\n",
    " * tensorflow receive batches data, compute all together\n",
    " * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFR-BiLSTM\n",
    " * need to pad both to give same longth input for world embedd and charact embedd.\n",
    " * put the time dimension on axis=1 for dynamic_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build concatenated word embedding\n",
    "\n",
    " * tf.reshape()\n",
    "   * x*y*z == x1*y1*z1 == a*b; product stay constant\n",
    "   * -1, that dimension be computed according\n",
    "   * [-1] : flatten\n",
    "   \n",
    " * the sub-LSTM first output the the embedding for all the word in batch at once\n",
    "     * the dynamic_rnn need\n",
    "         * shape[1] the element need to be x_i in each time step\n",
    "         * shape[0] be the batch_size\n",
    "         * shape[2] be the dimension for x_i\n",
    "         * and compute the state in batch.\n",
    " * when coding tensorflow, we should keep the shape of tensor like the code below every time \n",
    " * Thanks to the sequence_length argument, for the unvalid time steps, the dynamic_rnn passes the state through and outputs a vector of zeros.\n",
    "     * so the output of dynamic_rnn is also have same length(==max_length),but 0 vector for those invaild time steps.\n",
    "     * the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get batch of word embedding for sentences #######################################\n",
    "# pad sentence \n",
    "# shape = (batch size, max length of sentence in batch)\n",
    "word_ids = tf.placeholder(tf.int32, shape=[None, None])\n",
    "# shape = (batch size)\n",
    "sequence_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "# load embedding and pad character embedd ############\n",
    "L = tf.Variable(embeddings, dtype=tf.float32, trainable=False)\n",
    "# shape = (batch, sentence, word_vector_size)\n",
    "pretrained_embeddings = tf.nn.embedding_lookup(L, word_ids)\n",
    "\n",
    "\n",
    "\n",
    "# get batch of untrained character embedding for sentences-word-level #######################################\n",
    "# shape = (batch size, max length of sentence, max length of word)\n",
    "char_ids = tf.placeholder(tf.int32, shape=[None, None, None])\n",
    "# shape = (batch_size, max_length of sentence)\n",
    "word_lengths = tf.placeholder(tf.int32, shape=[None, None])\n",
    "# dynamic pad: pad to the maximum length in the batch. Thus, sentence length and word length will depend on the batch.\n",
    "# randomly initial the character embedding \n",
    "# 1. get character embeddings, using default xavier_initializer\n",
    "K = tf.get_variable(name=\"char_embeddings\", dtype=tf.float32,\n",
    "    shape=[nchars, dim_char])\n",
    "# shape = (batch, sentence, word, dim of char embeddings)\n",
    "char_embeddings = tf.nn.embedding_lookup(K, char_ids)\n",
    "\n",
    "\n",
    "\n",
    "# get trained of character embedding for sentences-word-level #######################################\n",
    "# need to reshape into the 3-D, and the middle dimension should be the time slot element\n",
    "# 2. put the time dimension on axis=1 for dynamic_rnn\n",
    "s = tf.shape(char_embeddings) # store old shape\n",
    "# shape = (batch x sentence, word, dim of char embeddings)\n",
    "char_embeddings = tf.reshape(char_embeddings, shape=[-1, s[-2], s[-1]])\n",
    "word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "# 3. bi lstm on chars\n",
    "cell_fw = tf.contrib.rnn.LSTMCell(char_hidden_size, state_is_tuple=True)\n",
    "cell_bw = tf.contrib.rnn.LSTMCell(char_hidden_size, state_is_tuple=True)\n",
    "_, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "    cell_bw, char_embeddings, sequence_length=word_lengths,\n",
    "    dtype=tf.float32)\n",
    "# shape = (batch x sentence, 2 x char_hidden_size)\n",
    "output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "# shape = (batch, sentence, 2 x char_hidden_size)\n",
    "char_rep = tf.reshape(output, shape=[-1, s[1], 2*char_hidden_size])\n",
    "\n",
    "\n",
    "# get final embedding for sentences-word-level #######################################\n",
    "# shape = (batch, sentence, 2 x char_hidden_size + word_vector_size)\n",
    "word_embeddings = tf.concat([pretrained_embeddings, char_rep], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentecnce-level LSTM\n",
    "* in charact BiLSTM we extract only the last state vs. in word BiLSTM we need states from every time step.\n",
    "   * output_states: A tuple (output_state_fw, output_state_bw) containing the forward and the backward final states of bidirectional rnn.\n",
    "   * output is also the h in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_fw = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "cell_bw = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "(output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "    cell_bw, word_embeddings, sequence_length=sequence_lengths,\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# ??shape = (batch, sentence, 2 x hidden_size)\n",
    "context_rep = tf.concat([output_fw, output_bw], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF \n",
    " * for each word that has a vector presentation, use a FCNN to compute a vector of scores for each **\"possible tag\"**\n",
    "      * need to know the n_possible_tag\n",
    " * from here two methods for decoding\n",
    "     * softmax: locally decision\n",
    "     * linear-chain CRF: use of the neighbooring tagging decisions.\n",
    " * could use tf.sequence_mask to deal with the invalid padden in output score\n",
    " * the stack of CRF and LSTM is one line easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a score vector for each word ###################################################\n",
    "# a FCNN to generate scores of all the tag for each word\n",
    "W = tf.get_variable(\"W\", shape=[2*self.config.hidden_size, self.config.ntags],\n",
    "                dtype=tf.float32)\n",
    "\n",
    "b = tf.get_variable(\"b\", shape=[self.config.ntags], dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "\n",
    "ntime_steps = tf.shape(context_rep)[1]\n",
    "# shape = (batch x sentence, 2 x hidden_size)\n",
    "context_rep_flat = tf.reshape(context_rep, [-1, 2*hidden_size])\n",
    "# shape = (batch x sentence, ntags)\n",
    "pred = tf.matmul(context_rep_flat, W) + b\n",
    "# shape = (batch, sentence, ntags)\n",
    "scores = tf.reshape(pred, [-1, ntime_steps, ntags])\n",
    "\n",
    "\n",
    "\n",
    "# real crf for loss #######################################################################\n",
    "# shape = (batch, sentence) , ture labels, trainsition _param is need for later prediciton\n",
    "labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\")\n",
    "\n",
    "log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "scores, labels, sequence_lengths)\n",
    "loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "# possible locally softmax loss #########################################\n",
    "# losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=labels)\n",
    "# # shape = (batch, sentence, nclasses)\n",
    "# mask = tf.sequence_mask(sequence_lengths)\n",
    "# # apply mask\n",
    "# losses = tf.boolean_mask(losses, mask)\n",
    "# loss = tf.reduce_mean(losses)\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# find the best sequence\n",
    "# shape = (sentence, nclasses)\n",
    "viterbi_sequence, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                                score, transition_params)\n",
    "\n",
    "# labels_pred = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question:\n",
    "    1. is the oupt also be padded in the same size? will check if later also use the \"sequence_lengths\" parameter\n",
    "      ntime_steps is a scala or a vector\n",
    "       Yes final ues sequence_lengths in crf and softmax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NER_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "class NER_Model(GENERIC_Model):\n",
    "    \"\"\"here is the class for NER specialized model\"\"\"\n",
    "   \n",
    "    # load config file \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        # setting configuration in GENERIC_MODEL class\n",
    "        super(NER_Model, self).__init__(config)\n",
    "        \n",
    "        # ????\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "    \n",
    "    \n",
    "    \"\"\"NER computational graph\"\"\"\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        \n",
    "        # shape = [batch_size, max_length of sentence in this batch]\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"word_ids\")\n",
    "        \n",
    "        # shape = [batch_size]\n",
    "        self.sentence_lengths = tf.placeholder(tf.int32, shpae=[None], name = \"sentence_lengths\")\n",
    "        \n",
    "        # shape =[batch_size, max_length of sentence, max_length of word]\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None], name = \"char_ids\")\n",
    "        \n",
    "        # shape = [batch_size, max_length of sentence]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shpae=[None, None], name = \"word_lengths\")\n",
    "        \n",
    "        # shape = [batch_size, max_length of sentence]\n",
    "        self.labels = tf.placeholder(tf.int32, shape = [None, None], name = \"labels\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # hyper parameters\n",
    "        # scalar\n",
    "        self.dropout = tf.placeholder(tf.float32, shape=[], name = \"dropout\")\n",
    "        # scalar\n",
    "        self.lr = tf.placeholder(tf.float32, shape=[], name = \"lr\")\n",
    "        \n",
    "   \n",
    "    def get_feed_dict(self, words, labels = None, lr = None, dropout = None):\n",
    "        \"\"\"\n",
    "        data pre-processing\n",
    "        from batch of sentences of words to word_ids + char_ids\n",
    "        \"\"\"\n",
    "        if self.config.use_chars:\n",
    "            char_ids, word_ids =zip(*words)\n",
    "            word_ids, sentence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok =0, nlevels =2)\n",
    "        else :\n",
    "            word_ids, sentence_lengths = pad_sequences(word_ids, 0)\n",
    "            \n",
    "        # feed dict\n",
    "        feed = {\n",
    "            self.word_ids : word_ids, \n",
    "            self.self.sentence_lengths: sentence_lengths\n",
    "        }\n",
    "        \n",
    "        if self.config.use_chars:\n",
    "            feed[self.char_ids] = char_ids\n",
    "            feed[self.word_lengths] = word_lengths\n",
    "            \n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "            \n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "        \n",
    "        if droput is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "            \n",
    "        return feed, sentence_lengths\n",
    "    \n",
    "    \n",
    "    def add_final_embedding_op(self):\n",
    "        \"\"\"\n",
    "        generate char embedding by run sub-LSTM\n",
    "        concatenate word embedding and char embedding to get the final embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        # pre-trained word embedding \n",
    "        with tf.variable_scope(\"words\"):\n",
    "            \n",
    "            _word_embeddings_lookup_table = tf.Variable(\n",
    "            self.config.embeddings,\n",
    "            name = \"_word_embeddings_lookup_table\",\n",
    "            dtype = tf.float32,\n",
    "            trainable = self.config.train_embeddings)\n",
    "            \n",
    "            # shape = [batch_size, max_length of sentence in this batch, word_embedding_size]\n",
    "            word_embeddings = tf.nn.embedding_lookup(\n",
    "            _word_embeddings_lookup_table,\n",
    "            self.words_ids,\n",
    "            name = \"word_embeddings\")\n",
    "                \n",
    "        # run sub-LSTM to got char embedding\n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            if self.config.use_chars:\n",
    "                # initial the char embedding table\n",
    "                char_embedding_table = tf.get_variable(\n",
    "                    name = \"char_embedding_table\",\n",
    "                    dtype = tf.float32,\n",
    "                    shape = [self.config.nchars, self.config.dim_char])\n",
    "                \n",
    "                char_embeddings = tf.nn.embedding_lookup(char_embedding_table,\n",
    "                                                        self.char_ids,\n",
    "                                                       name = \"char_embeddings\")\n",
    "               \n",
    "                # reshape the char_embeddings as the time dimension on axis=1 need for bidirectional_dynamic_rnn\n",
    "                shape = tf.shape(char_embeddings)\n",
    "                char_embeddings = tf.reshape(char_embedding,\n",
    "                                            shape = [-1, shape[-2], shape[-1]])\n",
    "                word_lengths = tf.reshape(self.word_lengths, shape =[shape[0]*shape[1]])\n",
    "                \n",
    "                # run LSTM on char, c_state and m_state\n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                                                 state_is_tuple = True, name = \"char_Cell_fw\")\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                                                 state_is_tuple = True, name = \"char_Cell_bw\")\n",
    "                _output = tf.nn.bidrectional_dynamic_rnn(cell_fw, cell_bw, char_embeddings, \n",
    "                                                        sequence_length = word_lengths, dtype = tf.float32)\n",
    "                \n",
    "                # concat the two direction vector\n",
    "                _, ((_, output_fw),(_, output_bw)) = _output\n",
    "                output = tf.concat([output_fw, output_bw], axis = -1)\n",
    "                \n",
    "                #shape = [batch size, max sentence length, char hidden size]\n",
    "                output = tf.reshape(output, shape = [shape[0], shape[1], 2*self.config.hidden_size_char])\n",
    "                \n",
    "                # concat the final embedding\n",
    "                word_embeddings = tf.concat([word_embedding, output], axis = -1)\n",
    "        \n",
    "        self.word_embeddings = tf.nn.dropout(word_embeddings, self.dropout)\n",
    "        \n",
    "    # Bi-LSTM run on sentence level\n",
    "    def add_LSTM(self):\n",
    "        with tf.variable_scope(\"Bi-LSTM\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,self.world_embeddings,\n",
    "                                                                       sequence_length = self.sentence_lengths, dtype  = tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis = -1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "            \n",
    "    # a FC layer to project   \n",
    "    def add_FCNN(self):\n",
    "        with tf.variable_scope(\"FCNN\"):\n",
    "            W = tf.get_variable(\"W\", dtype = tf.float32,\n",
    "                           shape = [2*self.config.hidden_size_lstm, self.config.ntags])\n",
    "            \n",
    "            b = tf.get_variable(\"b\", shape = [self.config.ntags], \n",
    "                                dtype = tf.float32, initializer = tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tp.shape(output)[1]\n",
    "            output = tf.reshape(output, shape = [-1, 2*self.config.hidden_size_lstm])\n",
    "            scores = tf.matmul(output, W) +b\n",
    "            self.scores = tf.reshape(scores, shape = [-1, nsteps, self.config.ntags])\n",
    "    \n",
    "    # softmax locally prediction\n",
    "    def add_Softmax(self):\n",
    "        if not self.config.use_crf:\n",
    "            self.label_preds = tf.cast(tf.argmax(self.scores, axis = -1), tf.int32)\n",
    "            \n",
    "        \n",
    "    def add_loss_op(self):\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                self.scores, self.labels, self.sentence_lengths)\n",
    "            self.trans_params = trans_params\n",
    "            \n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        # if predict locally\n",
    "        else: \n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logists = self.scores, labels = self.labels)\n",
    "            # mask the padding\n",
    "            mask = tf.sequence_mask(self.sentence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            \n",
    "        #  tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "    \n",
    "    # build NER graph \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_final_embedding_op()\n",
    "        self.add_LSTM()\n",
    "        self.add_FCNN()\n",
    "        if not self.config.use_crf:\n",
    "            self.add_Softmax() \n",
    "        self.add_loss_op()\n",
    "        \n",
    "        \n",
    "        self.add_train_op(self.config.lr_method, self.lf, self.loss, self.config.clip)\n",
    "        self.initialize_session()\n",
    "        \n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"need the words be converted into wordId first\"\"\"\n",
    "        \n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout = 1.0)\n",
    "        \n",
    "        # if crf enable\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition matrix\n",
    "            viterbi_sequences = []\n",
    "            scores, trans_params = self.sess.run(\n",
    "                [self.scores, self.trans_params], feed_dict = fd)\n",
    "            \n",
    "            # viterbi decode to find the best label sequence\n",
    "            for score, sentence_length in zip(scores, sentence_lengths):\n",
    "                score = score[:sentence_lengths]\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                    score, trans_params)\n",
    "                viterbi_sequences.append(viterbi_seq)\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "        # if use softmax \n",
    "        elif not self.config.use_crf:\n",
    "            label_preds = self.sess.run([self.label_preds], feed_dict =fd)\n",
    "            return label_preds, sequence_lengths\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predic(self, words_raw):\n",
    "        \"\"\"\n",
    "            convert raw words into wordId and call predict_batch\n",
    "            convect indx_tag to tag for output\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type (words(0)) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "    \n",
    "        return preds\n",
    "    \n",
    "    def run_epoch(self, train_set, eval_set, epochs):\n",
    "        \"\"\"run complete pass over trainset and devset (one epoch)\"\"\"\n",
    "        \n",
    "        batch_size = self.config.batch_size\n",
    "        ####\n",
    "        ####\n",
    "        \n",
    "        for i, (words,labels) in enumerate(minibathes(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                                      self.config.dropout)\n",
    "            _, train_loss, summary = self.sess.run([self.train_op, self.loss, self.merged],\n",
    "                                                  feed_dict = fd)\n",
    "\n",
    "        ####\n",
    "        ####\n",
    "        \n",
    "        metrics = self.run_evaluate(dev)\n",
    "        \n",
    "        ####\n",
    "        ####\n",
    "        return metrics[\"f1\"]\n",
    "    \n",
    "    def run_evaluate(self, test):\n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibathes(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "            \n",
    "            for lab, lab_pred, length in zip (labels, labels_pred, sequence_lengths):\n",
    "                lab = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs += [a==b for (a,b) in zip(lab, lab_pred)]\n",
    "                \n",
    "                lab_chunks      = set(get_chunks(lab, self.config.vocab_tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                                 self.config.vocab_tags))\n",
    "\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds   += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)   \n",
    "                \n",
    "        p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    \"\"\"deal with generic NN model op\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.sess = None\n",
    "        self.saver = None\n",
    "        \n",
    "    def add_train_op(self, lr_method, lr, loss, clip = -1):\n",
    "        \n",
    "        _lr_m = lr_method.lower() # lower to make sure\n",
    "        \n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            if _lr_m == 'adam': # sgd method\n",
    "                optimizer = tf.train.AdamOptimizer(lr)\n",
    "            elif _lr_m == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(lr)\n",
    "            elif _lr_m == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            elif _lr_m == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
    "\n",
    "            if clip > 0: # gradient clipping if clip is positive\n",
    "                grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
    "                grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, vs))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(loss) \n",
    "                \n",
    "    def initialize_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "    def train(self, train, dev):\n",
    "        best_score = 0\n",
    "        nepoch_no_imprv = 0\n",
    "        \n",
    "        for epoch in range(self.config.nepochs):\n",
    "            score = self.run_epoch(train, dev, epoch)\n",
    "            \n",
    "            # early stopping \n",
    "            if score >= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                best_score = score\n",
    "            else:\n",
    "                nepoch_no_imprv += 1\n",
    "                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
    "                    break\n",
    "                    \n",
    "    def evalute(self, test):\n",
    "        metrics = self.run_evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "class NER_Model(GENERIC_Model):\n",
    "    \"\"\"here is the class for NER specialized model\"\"\"\n",
    "   \n",
    "    # load config file \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        # setting configuration in GENERIC_MODEL class\n",
    "        super(NER_Model, self).__init__(config)\n",
    "        \n",
    "        # ????\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "    \n",
    "    \n",
    "    \"\"\"NER computational graph\"\"\"\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        \n",
    "        # shape = [batch_size, max_length of sentence in this batch]\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"word_ids\")\n",
    "        \n",
    "        # shape = [batch_size]\n",
    "        self.sentence_lengths = tf.placeholder(tf.int32, shpae=[None], name = \"sentence_lengths\")\n",
    "        \n",
    "        # shape =[batch_size, max_length of sentence, max_length of word]\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None], name = \"char_ids\")\n",
    "        \n",
    "        # shape = [batch_size, max_length of sentence]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shpae=[None, None], name = \"word_lengths\")\n",
    "        \n",
    "        # shape = [batch_size, max_length of sentence]\n",
    "        self.labels = tf.placeholder(tf.int32, shape = [None, None], name = \"labels\")\n",
    "        \n",
    "        \n",
    "#####################################################\n",
    "modified\n",
    "#####################################################        \n",
    "        # hyper parameters\n",
    "        # scalar\n",
    "        self.dropout = tf.placeholder(tf.float32, shape=[], name = \"dropout\")\n",
    "        # scalar\n",
    "        self.lr = tf.placeholder(tf.float32, shape=[], name = \"lr\")\n",
    "#####################################################\n",
    "modified\n",
    "#####################################################\n",
    "\n",
    "\n",
    "   \n",
    "    def get_feed_dict(self, words, labels = None, lr = None, dropout = None):\n",
    "        \"\"\"\n",
    "        data pre-processing\n",
    "        from batch of sentences of words to word_ids + char_ids\n",
    "        \"\"\"\n",
    "        if self.config.use_chars:\n",
    "            char_ids, word_ids =zip(*words)\n",
    "            word_ids, sentence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok =0, nlevels =2)\n",
    "        else :\n",
    "            word_ids, sentence_lengths = pad_sequences(word_ids, 0)\n",
    "            \n",
    "        # feed dict\n",
    "        feed = {\n",
    "            self.word_ids : word_ids, \n",
    "            self.self.sentence_lengths: sentence_lengths\n",
    "        }\n",
    "        \n",
    "        if self.config.use_chars:\n",
    "            feed[self.char_ids] = char_ids\n",
    "            feed[self.word_lengths] = word_lengths\n",
    "            \n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "            \n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "        \n",
    "        if droput is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "            \n",
    "        return feed, sentence_lengths\n",
    "    \n",
    "    \n",
    "    def add_final_embedding_op(self):\n",
    "        \"\"\"\n",
    "        generate char embedding by run sub-LSTM\n",
    "        concatenate word embedding and char embedding to get the final embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        # pre-trained word embedding \n",
    "        with tf.variable_scope(\"words\"):\n",
    "            \n",
    "            _word_embeddings_lookup_table = tf.Variable(\n",
    "            self.config.embeddings,\n",
    "            name = \"_word_embeddings_lookup_table\",\n",
    "            dtype = tf.float32,\n",
    "            trainable = self.config.train_embeddings)\n",
    "            \n",
    "            # shape = [batch_size, max_length of sentence in this batch, word_embedding_size]\n",
    "            word_embeddings = tf.nn.embedding_lookup(\n",
    "            _word_embeddings_lookup_table,\n",
    "            self.words_ids,\n",
    "            name = \"word_embeddings\")\n",
    "                \n",
    "        # run sub-LSTM to got char embedding\n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            if self.config.use_chars:\n",
    "                # initial the char embedding table\n",
    "                char_embedding_table = tf.get_variable(\n",
    "                    name = \"char_embedding_table\",\n",
    "                    dtype = tf.float32,\n",
    "                    shape = [self.config.nchars, self.config.dim_char])\n",
    "                \n",
    "                char_embeddings = tf.nn.embedding_lookup(char_embedding_table,\n",
    "                                                        self.char_ids,\n",
    "                                                       name = \"char_embeddings\")\n",
    "               \n",
    "                # reshape the char_embeddings as the time dimension on axis=1 need for bidirectional_dynamic_rnn\n",
    "                shape = tf.shape(char_embeddings)\n",
    "                char_embeddings = tf.reshape(char_embedding,\n",
    "                                            shape = [-1, shape[-2], shape[-1]])\n",
    "                word_lengths = tf.reshape(self.word_lengths, shape =[shape[0]*shape[1]])\n",
    "                \n",
    "                # run LSTM on char, c_state and m_state\n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                                                 state_is_tuple = True, name = \"char_Cell_fw\")\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                                                 state_is_tuple = True, name = \"char_Cell_bw\")\n",
    "                _output = tf.nn.bidrectional_dynamic_rnn(cell_fw, cell_bw, char_embeddings, \n",
    "                                                        sequence_length = word_lengths, dtype = tf.float32)\n",
    "                \n",
    "                # concat the two direction vector\n",
    "                _, ((_, output_fw),(_, output_bw)) = _output\n",
    "                output = tf.concat([output_fw, output_bw], axis = -1)\n",
    "                \n",
    "                #shape = [batch size, max sentence length, char hidden size]\n",
    "                output = tf.reshape(output, shape = [shape[0], shape[1], 2*self.config.hidden_size_char])\n",
    "                \n",
    "                # concat the final embedding\n",
    "                word_embeddings = tf.concat([word_embedding, output], axis = -1)\n",
    "        \n",
    "        self.word_embeddings = tf.nn.dropout(word_embeddings, self.dropout)\n",
    "        \n",
    "    # Bi-LSTM run on sentence level\n",
    "    def add_LSTM(self):\n",
    "        with tf.variable_scope(\"Bi-LSTM\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,self.world_embeddings,\n",
    "                                                                       sequence_length = self.sentence_lengths, dtype  = tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis = -1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "            \n",
    "    # a FC layer to project   \n",
    "    def add_FCNN(self):\n",
    "        with tf.variable_scope(\"FCNN\"):\n",
    "            W = tf.get_variable(\"W\", dtype = tf.float32,\n",
    "                           shape = [2*self.config.hidden_size_lstm, self.config.ntags])\n",
    "            \n",
    "            b = tf.get_variable(\"b\", shape = [self.config.ntags], \n",
    "                                dtype = tf.float32, initializer = tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tp.shape(output)[1]\n",
    "            output = tf.reshape(output, shape = [-1, 2*self.config.hidden_size_lstm])\n",
    "            scores = tf.matmul(output, W) +b\n",
    "            self.scores = tf.reshape(scores, shape = [-1, nsteps, self.config.ntags])\n",
    "    \n",
    "    # softmax locally prediction\n",
    "    def add_Softmax(self):\n",
    "        if not self.config.use_crf:\n",
    "            self.label_preds = tf.cast(tf.argmax(self.scores, axis = -1), tf.int32)\n",
    "            \n",
    "        \n",
    "    def add_loss_op(self):\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                self.scores, self.labels, self.sentence_lengths)\n",
    "            self.trans_params = trans_params\n",
    "            \n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        # if predict locally\n",
    "        else: \n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logists = self.scores, labels = self.labels)\n",
    "            # mask the padding\n",
    "            mask = tf.sequence_mask(self.sentence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            \n",
    "        #  tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "    \n",
    "    # build NER graph \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_final_embedding_op()\n",
    "        self.add_LSTM()\n",
    "        self.add_FCNN()\n",
    "        if not self.config.use_crf:\n",
    "            self.add_Softmax() \n",
    "        self.add_loss_op()\n",
    "        \n",
    "        \n",
    "        self.add_train_op(self.config.lr_method, self.lf, self.loss, self.config.clip)\n",
    "        self.initialize_session()\n",
    "        \n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"need the words be converted into wordId first\"\"\"\n",
    "        \n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout = 1.0)\n",
    "        \n",
    "        # if crf enable\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition matrix\n",
    "            viterbi_sequences = []\n",
    "            scores, trans_params = self.sess.run(\n",
    "                [self.scores, self.trans_params], feed_dict = fd)\n",
    "            \n",
    "            # viterbi decode to find the best label sequence\n",
    "            for score, sentence_length in zip(scores, sentence_lengths):\n",
    "                score = score[:sentence_lengths]\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                    score, trans_params)\n",
    "                viterbi_sequences.append(viterbi_seq)\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "        # if use softmax \n",
    "        elif not self.config.use_crf:\n",
    "            label_preds = self.sess.run([self.label_preds], feed_dict =fd)\n",
    "            return label_preds, sequence_lengths\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predic(self, words_raw):\n",
    "        \"\"\"\n",
    "            convert raw words into wordId and call predict_batch\n",
    "            convect indx_tag to tag for output\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type (words(0)) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "    \n",
    "        return preds\n",
    "    \n",
    "    def run_epoch(self, train_set, eval_set, epochs):\n",
    "        \"\"\"run complete pass over trainset and devset (one epoch)\"\"\"\n",
    "        \n",
    "        batch_size = self.config.batch_size\n",
    "        ####\n",
    "        ####\n",
    "        \n",
    "        for i, (words,labels) in enumerate(minibathes(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                                      self.config.dropout)\n",
    "            _, train_loss, summary = self.sess.run([self.train_op, self.loss, self.merged],\n",
    "                                                  feed_dict = fd)\n",
    "\n",
    "        ####\n",
    "        ####\n",
    "        \n",
    "        metrics = self.run_evaluate(dev)\n",
    "        \n",
    "        ####\n",
    "        ####\n",
    "        return metrics[\"f1\"]\n",
    "    \n",
    "    def run_evaluate(self, test):\n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibathes(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "            \n",
    "            for lab, lab_pred, length in zip (labels, labels_pred, sequence_lengths):\n",
    "                lab = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs += [a==b for (a,b) in zip(lab, lab_pred)]\n",
    "                \n",
    "                lab_chunks      = set(get_chunks(lab, self.config.vocab_tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                                 self.config.vocab_tags))\n",
    "\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds   += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)   \n",
    "                \n",
    "        p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    \"\"\"deal with generic NN model op\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.sess = None\n",
    "        self.saver = None\n",
    "        \n",
    "    def add_train_op(self, lr_method, lr, loss, clip = -1):\n",
    "        \n",
    "        _lr_m = lr_method.lower() # lower to make sure\n",
    "        \n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            if _lr_m == 'adam': # sgd method\n",
    "                optimizer = tf.train.AdamOptimizer(lr)\n",
    "            elif _lr_m == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(lr)\n",
    "            elif _lr_m == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            elif _lr_m == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
    "\n",
    "            if clip > 0: # gradient clipping if clip is positive\n",
    "                grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
    "                grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, vs))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(loss) \n",
    "                \n",
    "    def initialize_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "    def train(self, train, dev):\n",
    "        best_score = 0\n",
    "        nepoch_no_imprv = 0\n",
    "        \n",
    "        for epoch in range(self.config.nepochs):\n",
    "            score = self.run_epoch(train, dev, epoch)\n",
    "            \n",
    "            # early stopping \n",
    "            if score >= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                best_score = score\n",
    "            else:\n",
    "                nepoch_no_imprv += 1\n",
    "                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
    "                    break\n",
    "                    \n",
    "    def evalute(self, test):\n",
    "        metrics = self.run_evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    # training\n",
    "    train_embeddings = False\n",
    "    n_epochs          = 15\n",
    "    dropout          = 0.5\n",
    "    batch_size       = 20\n",
    "    lr_method        = \"adam\"\n",
    "    lr               = 0.001\n",
    "    lr_decay         = 0.9\n",
    "    clip             = -1 # if negative, no clipping\n",
    "    n_epoch_no_imprv  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"CRF-BiLSTM NER model\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "\t\tLoad the hyperparams in config\n",
    "\n",
    "        \"\"\"\n",
    "        self.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = Model(Config)\n",
    "test_model.config.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
