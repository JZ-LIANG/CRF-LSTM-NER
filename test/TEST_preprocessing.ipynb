{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Simplied method:\n",
    " * buil vocabu from pre-trained embedding instead of corpus\n",
    " * don't distinct the Unknow words, all map to single UNK\n",
    "     * embedding_table[0] = PAD;  embedding_table[-1] = UNK;\n",
    " * Uncased embedding\n",
    " * limit the vocabulary size\n",
    " * tf.embedding_lookup\n",
    " * Data structure\n",
    "     * word2idx :  dictionary for mapping words to their index token - used for converting a sequence of words to sequence of integers for embedding lookup\n",
    "     * idx2word : a list of words in order - used for decoding an integer sequence to words\n",
    "     * weights : a matrice of size VOCAB_LENGTH x EMBEDDING_DIMESNION containing the vectors for each word\n",
    "https://www.damienpontifex.com/2017/10/27/using-pre-trained-glove-embeddings-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available dimensions for 6B data is 50, 100, 200, 300\n",
    "EMBEDDING_DIMENSION=50 \n",
    "data_directory = '../data/glove'\n",
    "\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "\n",
    "glove_weights_file_path = os.path.join(data_directory, 'glove.6B.{}d.txt'.format(MBEDDING_DIMENSION))\n",
    "\n",
    "# if not glove files, download if\n",
    "if not os.path.isfile(glove_weights_file_path):\n",
    "    # Glove embedding weights can be downloaded from https://nlp.stanford.edu/projects/glove/\n",
    "    glove_fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    local_zip_file_path = os.path.join(data_directory, os.path.basename(glove_fallback_url))\n",
    "    if not os.path.isfile(local_zip_file_path):\n",
    "        print('Retreiving glove weights from {}'.format(fallback_url))\n",
    "        urllib.request.urlretrieve(glove_fallback_url, local_zip_file_path)\n",
    "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
    "        print('Extracting glove weights from {}'.format(local_zip_file_path))\n",
    "        z.extractall(path=data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'PAD' the only uppercase word\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "# dict so we can lookup indices for tokenising our text later from string to sequence of integers weights = []\n",
    "word2idx = { 'PAD': PAD_TOKEN } \n",
    "weights = []\n",
    "idx2word = []\n",
    "\n",
    "\n",
    "with open(glove_weights_file_path, 'r') as file:     \n",
    "    for index, line in enumerate(file): \n",
    "        values = line.split()\n",
    "        # Word and weights separated by space \n",
    "        word = values[0]\n",
    "        # Word is first symbol on each line \n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32) \n",
    "        # Remainder of line is weights for word \n",
    "        word2idx[word] = index + 1 \n",
    "        # Remainder of line is weights for word \n",
    "        weights.append(word_weights)\n",
    "        # update the idx2word\n",
    "        idx2word.append(word)\n",
    "        \n",
    "        if index + 1 == 40000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n",
    "            \n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "idx2word.insert(0,'PAD')\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random\n",
    "UNKNOWN_TOKEN=len(weights) \n",
    "word2idx['UNK'] = UNKNOWN_TOKEN \n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION))\n",
    "idx2word.append('UNK')\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_indices': [13076, 86]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings in TensorFlow\n",
    "features = {}\n",
    "features['word_indices'] = nltk.word_tokenize('hello world') # ['hello', 'world']\n",
    "features['word_indices'] = [word2idx.get(word, UNKNOWN_TOKEN) for word in features['word_indices']]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      "  -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "   0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "   0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "   0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      "  -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      "  -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "   0.67204 ]\n",
      " [-0.41486   0.71848  -0.3045    0.87445   0.22441  -0.56488  -0.37566\n",
      "  -0.44801   0.61347  -0.11359   0.74556  -0.10598  -1.1882    0.50974\n",
      "   1.3511    0.069851  0.73314   0.26773  -1.1787   -0.148     0.039853\n",
      "   0.033107 -0.27406   0.25125   0.41507  -1.6188   -0.81778  -0.73892\n",
      "  -0.28997   0.57277   3.4719    0.73817  -0.044495 -0.15119  -0.93503\n",
      "  -0.13152  -0.28562   0.76327  -0.83332  -0.6793   -0.39099  -0.64466\n",
      "   1.0044   -0.2051    0.46799   0.99314  -0.16221  -0.46022  -0.37639\n",
      "  -0.67542 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "glove_weights_initializer = tf.constant_initializer(weights)\n",
    "embedding_weights = tf.get_variable(\n",
    "    name='embedding_weights', \n",
    "    shape=(VOCAB_SIZE, EMBEDDING_DIMENSION), \n",
    "    initializer=glove_weights_initializer,\n",
    "    trainable=False)\n",
    "embedding = tf.nn.embedding_lookup(embedding_weights, features['word_indices'])\n",
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# check idx2word\n",
    "print(idx2word[13076], idx2word[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2rd Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://dashayushman.github.io/tutorials/2017/08/19/neural-language-model.html\n",
    "https://medium.com/@TalPerry/getting-text-into-tensorflow-with-the-dataset-api-ffb832c8bec6\n",
    "    https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "https://machinelearnings.co/tensorflow-text-classification-615198df9231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
