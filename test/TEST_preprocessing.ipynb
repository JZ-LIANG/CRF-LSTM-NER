{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method1: build vocabu from embedding table\n",
    " * buil vocabu from pre-trained embedding instead of corpus\n",
    " * don't distinct the Unknow words, all map to single UNK\n",
    "     * embedding_table[0] = PAD;  embedding_table[-1] = UNK;\n",
    " * Uncased embedding\n",
    " * limit the vocabulary size\n",
    " * tf.embedding_lookup\n",
    " * Data structure\n",
    "     * word2idx :  dictionary for mapping words to their index token - used for converting a sequence of words to sequence of integers for embedding lookup\n",
    "     * idx2word : a list of words in order - used for decoding an integer sequence to words\n",
    "     * weights : a matrice of size VOCAB_LENGTH x EMBEDDING_DIMESNION containing the vectors for each word\n",
    "https://www.damienpontifex.com/2017/10/27/using-pre-trained-glove-embeddings-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Available dimensions for 6B data is 50, 100, 200, 300\n",
    "EMBEDDING_DIMENSION=50 \n",
    "data_directory = '../data/glove'\n",
    "\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "\n",
    "glove_weights_file_path = os.path.join(data_directory, 'glove.6B.{}d.txt'.format(MBEDDING_DIMENSION))\n",
    "\n",
    "# if not glove files, download if\n",
    "if not os.path.isfile(glove_weights_file_path):\n",
    "    # Glove embedding weights can be downloaded from https://nlp.stanford.edu/projects/glove/\n",
    "    glove_fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    local_zip_file_path = os.path.join(data_directory, os.path.basename(glove_fallback_url))\n",
    "    if not os.path.isfile(local_zip_file_path):\n",
    "        print('Retreiving glove weights from {}'.format(fallback_url))\n",
    "        urllib.request.urlretrieve(glove_fallback_url, local_zip_file_path)\n",
    "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
    "        print('Extracting glove weights from {}'.format(local_zip_file_path))\n",
    "        z.extractall(path=data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add 'PAD' the only uppercase word\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "# dict so we can lookup indices for tokenising our text later from string to sequence of integers weights = []\n",
    "word2idx = { 'PAD': PAD_TOKEN } \n",
    "weights = []\n",
    "idx2word = []\n",
    "\n",
    "\n",
    "with open(glove_weights_file_path, 'r') as file:     \n",
    "    for index, line in enumerate(file): \n",
    "        values = line.split()\n",
    "        # Word and weights separated by space \n",
    "        word = values[0]\n",
    "        # Word is first symbol on each line \n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32) \n",
    "        # Remainder of line is weights for word \n",
    "        word2idx[word] = index + 1 \n",
    "        # Remainder of line is weights for word \n",
    "        weights.append(word_weights)\n",
    "        # update the idx2word\n",
    "        idx2word.append(word)\n",
    "        \n",
    "        if index + 1 == 40000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n",
    "            \n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "idx2word.insert(0,'PAD')\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random\n",
    "UNKNOWN_TOKEN=len(weights) \n",
    "word2idx['UNK'] = UNKNOWN_TOKEN \n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION))\n",
    "idx2word.append('UNK')\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_indices': [13076, 86]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings in TensorFlow\n",
    "features = {}\n",
    "features['word_indices'] = nltk.word_tokenize('hello world') # ['hello', 'world']\n",
    "features['word_indices'] = [word2idx.get(word, UNKNOWN_TOKEN) for word in features['word_indices']]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      "  -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "   0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "   0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "   0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      "  -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      "  -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "   0.67204 ]\n",
      " [-0.41486   0.71848  -0.3045    0.87445   0.22441  -0.56488  -0.37566\n",
      "  -0.44801   0.61347  -0.11359   0.74556  -0.10598  -1.1882    0.50974\n",
      "   1.3511    0.069851  0.73314   0.26773  -1.1787   -0.148     0.039853\n",
      "   0.033107 -0.27406   0.25125   0.41507  -1.6188   -0.81778  -0.73892\n",
      "  -0.28997   0.57277   3.4719    0.73817  -0.044495 -0.15119  -0.93503\n",
      "  -0.13152  -0.28562   0.76327  -0.83332  -0.6793   -0.39099  -0.64466\n",
      "   1.0044   -0.2051    0.46799   0.99314  -0.16221  -0.46022  -0.37639\n",
      "  -0.67542 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "glove_weights_initializer = tf.constant_initializer(weights)\n",
    "embedding_weights = tf.get_variable(\n",
    "    name='embedding_weights', \n",
    "    shape=(VOCAB_SIZE, EMBEDDING_DIMENSION), \n",
    "    initializer=glove_weights_initializer,\n",
    "    trainable=False)\n",
    "embedding = tf.nn.embedding_lookup(embedding_weights, features['word_indices'])\n",
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# check idx2word\n",
    "print(idx2word[13076], idx2word[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2rd Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://dashayushman.github.io/tutorials/2017/08/19/neural-language-model.html\n",
    "https://medium.com/@TalPerry/getting-text-into-tensorflow-with-the-dataset-api-ffb832c8bec6\n",
    "    https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "https://machinelearnings.co/tensorflow-text-classification-615198df9231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spacy for most of pre-processing\n",
    "    *normalization \n",
    "        * lowercase, stemming lemmatization \n",
    "    * SEQUENCE_BEGIN and SEQUENCE_END\n",
    "* create vocabulary from corpus\n",
    "    * dict\n",
    "        * Word2Idx: This dictionary has all the unique words(terms) as keys with a corresponding unique ID as values\n",
    "        * Idx2Word: This is the reverse of Word2Idx. It has the unique IDs as keys and their corresponding words(terms) as values\n",
    "* **collections library**\n",
    "* count term **frequencies**, and to select the most commonly occurring terms in the vocabulary (as it covers most of the Natural Language).\n",
    "* Uniform UNK\n",
    "    * random initial the word not in pre-trained but in training corpus\n",
    "    * not accept all the word in pre-trained\n",
    "    * word not in training corpus but in test corpus are as UNK\n",
    "    \n",
    "* \"0\" pad\n",
    "* **!! how to match the index according to frequency and the index of pre-trained table?? ** : re-extract it\n",
    "\n",
    "* step : frequencey --> vocabu --> Word2Idx + Idx2Word --> re-build matched table.\n",
    "         sentence --> token --> idx token -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = ['today', 'generative', 'stimuli', 'loosely', 'natural', 'at', 'level', 'opposed', 'experts', \n",
    "         'be', 'broader', 'comparable', 'propositional', 'also', 'language', 'wealth', 'such', 'responses', \n",
    "         'methods', 'continued', 'Colorado', 'correspond', 'leading', 'CAP', 'part', 'finance', 'valid', 'competed', \n",
    "         'in', 'or', 'composition', 'discoveries', 'recurrent', 'patterns', 'algorithms', 'biological', 'neural', \n",
    "         'some', 'networks', 'network', 'cascade', 'many', 'analysis', 'agree', 'Drinker', ':', 'theft', 'family', \n",
    "         'filtering', 'hidden', 'Othniel', 'massive', 'Bone', 'are', 'efficient', 'these', 'Cope', 'remain', 'America',\n",
    "         'may', 'depends', 'levels', ']', 'solved', 'system', 'sought', 'high', '-', 'divides', 'disgrace', 'once', \n",
    "         'assignment', 'excavation', 'that', 'Academy', 'dinosaurs', 'low', 'rich', 'descriptions', 'results', 'human',\n",
    "         'threshold', 'computer', 'previous', 'than', 'has', 'Deep', 'history', 'using', 'light', 'resorting', \n",
    "         'include', 'SEQUENCE_BEGIN', 'mainly', 'bones', 'applied', 'Belief', 'processing', 'no', 'dinosaur', 'define',\n",
    "         'prehistoric', 'destruction', 'been', 'recognition', 'Marsh', 'speech', 'attempts', 'coding', 'each', 'depth', \n",
    "         'derived', 'ruined', 'potentially', 'representations', 'Wars', 'superior', 'features.[8', 'abstraction', \n",
    "         'information', 'brain', 'researchers', 'used', 'but', 'describe', 'complicated', 'own', 'paleontologists', \n",
    "         'species', 'systems', 'hierarchy', 'shed', 'research', 'caps', 'class', 'form', 'translation', 'Museum', \n",
    "         'pattern', 'path', 'Edward', 'transformations', 'socially', 'The', 'other', 'create', 'features', 'lower', \n",
    "         'fossils', 'concepts', 'neuronal', 'deaths', 'life', 'influence', 'fields', 'layers', 'different', 'audio', \n",
    "         'scientific', 'procure', 'layer', 'is', 'algorithm', 'applications', 'Machines', 'plus', 'upon', 'latent', \n",
    "         'representation', 'connections', '1892', 'contributions', 'science', 'through', 'definitions', 'hunters', \n",
    "         'organized', 'shallow', 'transformation', 'data', 'partially', 'wise', 'number', 'feedforward', 'bone', \n",
    "         'variables', 'Philadelphia', 'age', 'they', 'to', 'nodes', 'belief', 'universally', 'including', 'by', \n",
    "         'wars', 'of', 'where', 'unsupervised', 'architectures', 'Networks', '/', 'fossil', 'American', ',', 'and',\n",
    "         'artificial', 'more', 'boxes', 'interest', 'propagate', 'follow', 'Yale', 'have', 'problem', 'efforts', \n",
    "         'Sciences', 'new', 'task', 'bribery', 'beds', 'associated', 'for', 'hierarchical', 'were', 'sparked', \n",
    "         'scale', 'cases', 'paleontology', 'the', 'Wyoming', 'interpretation', 'classification', 'multiple', 'an', \n",
    "         'higher', 'forming', 'agreed', 'surge', 'nervous', 'chain', 'Peabody', 'Charles', 'causal', '32', '.', 'deep',\n",
    "         'vision', 'services', 'unlimited', 'unlabeled', 'formulas.[9', 'bioinformatics', 'uses', 'successive', 'one',\n",
    "         'on', 'produced', 'cap', 'specific', 'rivalries', 'sets', 'as', 'financially', 'feature', 'large', 'a', 'most',\n",
    "         'from', 'during', 'History', 'based', 'underhanded', 'Nebraska', 'expeditions', 'with', 'relationship', \n",
    "         'Boltzmann', 'credit', 'communication', 'â€“', 'found', 'North', 'learning', ';', 'led', 'SEQUENCE_END', \n",
    "         'between', 'after', 'models', 'machine', 'extraction', 'unopened', \"'s\", 'learn', 'input', 'decades', 'their', \n",
    "         'social', '1877', 'Natural', 'various', 'common', 'gilded', 'mutual', 'publications', 'public', 'can', \n",
    "         'supervised', 'field', 'use', 'output', 'nonlinear', 'signal', 'attacks', 'which', 'units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count frequency\n",
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for term in corpus_tokens:\n",
    "    word_counter.update({term: 1})\n",
    "vocab = word_counter.most_common(200) # 200 Most common terms\n",
    "print('Vocab Size: {}'.format(len(vocab))) \n",
    "print(word_counter.most_common(100)) # just to show the top 100 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UNKNOWN and PAD\n",
    "vocab.append(('UNKNOWN', 1))\n",
    "Idx = range(1, len(vocab)+1)\n",
    "vocab = [t[0] for t in vocab]\n",
    "\n",
    "# how to build dict\n",
    "Word2Idx = dict(zip(vocab, Idx))\n",
    "Idx2Word = dict(zip(Idx, vocab))\n",
    "\n",
    "# zero for PAD\n",
    "Word2Idx['PAD'] = 0\n",
    "Idx2Word[0] = 'PAD'\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(Word2Idx)\n",
    "print('Word2Idx Size: {}'.format(len(Word2Idx)))\n",
    "print('Idx2Word Size: {}'.format(len(Idx2Word)))\n",
    "print(Word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-extract pre-trained table to make index same order\n",
    "# if one word not in pre-trained, it is ramdon initial\n",
    "w2v = np.random.rand(len(Word2Idx), 300) # We use 300 because Spacy provides us with vectors of size 300\n",
    "\n",
    "for w_i, key in enumerate(Word2Idx):\n",
    "    token = nlp(key[0])\n",
    "    if token.has_vector:\n",
    "        w2v[w_i:] = token.vector\n",
    "EMBEDDING_SIZE = w2v.shape[-1]\n",
    "print('Shape of w2v: {}'.format(w2v.shape))\n",
    "print('Some Vectors')\n",
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ba50fe7ee45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_id_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2idseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvalidation_id_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2idseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# A method to convert a sequence of words into a sequence of IDs given a Word2Idx dictionary\n",
    "def word2idseq(data, word2idx):\n",
    "    id_seq = []\n",
    "    for word in data:\n",
    "        if word in word2idx:\n",
    "            id_seq.append(word2idx[word])\n",
    "        else:\n",
    "            id_seq.append(word2idx['UNKNOWN'])\n",
    "    return id_seq\n",
    "\n",
    "# Thanks to http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "# This method generated n-grams\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "train_id_seqs = word2idseq(train, Word2Idx)\n",
    "validation_id_seqs = word2idseq(validation, Word2Idx)\n",
    "\n",
    "print('Sample Train IDs')\n",
    "print(train_id_seqs[-10:-1])\n",
    "print('Sample Validation IDs')\n",
    "print(validation_id_seqs[-10:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Method 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import utils\n",
    "import collections\n",
    "import codecs\n",
    "import utils_nlp\n",
    "import re\n",
    "import time\n",
    "import token\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parse_dataset(self, dataset_filepath):\n",
    "    token_count = collections.defaultdict(lambda: 0)\n",
    "    label_count = collections.defaultdict(lambda: 0)\n",
    "    character_count = collections.defaultdict(lambda: 0)\n",
    "\n",
    "    line_count = -1\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    new_token_sequence = []\n",
    "    new_label_sequence = []\n",
    "    if dataset_filepath:\n",
    "        f = codecs.open(dataset_filepath, 'r', 'UTF-8')\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) == 0 or len(line[0]) == 0 or '-DOCSTART-' in line[0]:\n",
    "                if len(new_token_sequence) > 0:\n",
    "                    labels.append(new_label_sequence)\n",
    "                    tokens.append(new_token_sequence)\n",
    "                    new_token_sequence = []\n",
    "                    new_label_sequence = []\n",
    "                continue\n",
    "            token = str(line[0])\n",
    "            label = str(line[-1])\n",
    "            token_count[token] += 1\n",
    "            label_count[label] += 1\n",
    "\n",
    "            new_token_sequence.append(token)\n",
    "            new_label_sequence.append(label)\n",
    "\n",
    "            for character in token:\n",
    "                character_count[character] += 1\n",
    "\n",
    "            if self.debug and line_count > 200: break# for debugging purposes\n",
    "\n",
    "        if len(new_token_sequence) > 0:\n",
    "            labels.append(new_label_sequence)\n",
    "            tokens.append(new_token_sequence)\n",
    "        f.close()\n",
    "    return labels, tokens, token_count, label_count, character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    token_to_vector = {}\n",
    "    all_tokens_in_pretraining_dataset = []\n",
    "    all_characters_in_pretraining_dataset = []\n",
    "\n",
    "    \n",
    "    remap_to_unk_count_threshold = 1\n",
    "    self.UNK_TOKEN_INDEX = 0\n",
    "    self.PADDING_CHARACTER_INDEX = 0\n",
    "    self.tokens_mapped_to_unk = []\n",
    "    self.UNK = 'UNK'\n",
    "    self.unique_labels = []\n",
    "    labels = {}\n",
    "    tokens = {}\n",
    "    label_count = {}\n",
    "    token_count = {}\n",
    "    character_count = {}\n",
    "    \n",
    "    ####!!!!!!######\n",
    "    for dataset_type in ['train', 'valid', 'test', 'deploy']:\n",
    "        labels[dataset_type], tokens[dataset_type], token_count[dataset_type], label_count[dataset_type], character_count[dataset_type] \\\n",
    "                = self._parse_dataset(dataset_filepaths.get(dataset_type, None))\n",
    "    \n",
    "    \n",
    "    token_count['all'] = {}\n",
    "    for token in list(token_count['train'].keys()) + list(token_count['valid'].keys()) + list(token_count['test'].keys()) + list(token_count['deploy'].keys()):\n",
    "        token_count['all'][token] = token_count['train'][token] + token_count['valid'][token] + token_count['test'][token] + token_count['deploy'][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(self, dataset_filepaths, parameters, token_to_vector=None):\n",
    "        '''\n",
    "        dataset_filepaths : dictionary with keys 'train', 'valid', 'test', 'deploy'\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "        print('Load dataset... ', end='', flush=True)\n",
    "        if parameters['token_pretrained_embedding_filepath'] != '':\n",
    "            if token_to_vector==None:\n",
    "                token_to_vector = utils_nlp.load_pretrained_token_embeddings(parameters)\n",
    "        else:\n",
    "            token_to_vector = {}\n",
    "        if self.verbose: print(\"len(token_to_vector): {0}\".format(len(token_to_vector)))\n",
    "\n",
    "        # Load pretraining dataset to ensure that index to label is compatible to the pretrained model,\n",
    "        #   and that token embeddings that are learned in the pretrained model are loaded properly.\n",
    "        all_tokens_in_pretraining_dataset = []\n",
    "        all_characters_in_pretraining_dataset = []\n",
    "        if parameters['use_pretrained_model']:\n",
    "            pretraining_dataset = pickle.load(open(os.path.join(parameters['pretrained_model_folder'], 'dataset.pickle'), 'rb'))\n",
    "            all_tokens_in_pretraining_dataset = pretraining_dataset.index_to_token.values()\n",
    "            all_characters_in_pretraining_dataset = pretraining_dataset.index_to_character.values()\n",
    "\n",
    "        remap_to_unk_count_threshold = 1\n",
    "        self.UNK_TOKEN_INDEX = 0\n",
    "        self.PADDING_CHARACTER_INDEX = 0\n",
    "        self.tokens_mapped_to_unk = []\n",
    "        self.UNK = 'UNK'\n",
    "        self.unique_labels = []\n",
    "        labels = {}\n",
    "        tokens = {}\n",
    "        label_count = {}\n",
    "        token_count = {}\n",
    "        character_count = {}\n",
    "        for dataset_type in ['train', 'valid', 'test', 'deploy']:\n",
    "            labels[dataset_type], tokens[dataset_type], token_count[dataset_type], label_count[dataset_type], character_count[dataset_type] \\\n",
    "                = self._parse_dataset(dataset_filepaths.get(dataset_type, None))\n",
    "\n",
    "            if self.verbose: print(\"dataset_type: {0}\".format(dataset_type))\n",
    "            if self.verbose: print(\"len(token_count[dataset_type]): {0}\".format(len(token_count[dataset_type])))\n",
    "\n",
    "        token_count['all'] = {}\n",
    "        for token in list(token_count['train'].keys()) + list(token_count['valid'].keys()) + list(token_count['test'].keys()) + list(token_count['deploy'].keys()):\n",
    "            token_count['all'][token] = token_count['train'][token] + token_count['valid'][token] + token_count['test'][token] + token_count['deploy'][token]\n",
    "        \n",
    "        if parameters['load_all_pretrained_token_embeddings']:\n",
    "            for token in token_to_vector:\n",
    "                if token not in token_count['all']:\n",
    "                    token_count['all'][token] = -1\n",
    "                    token_count['train'][token] = -1\n",
    "            for token in all_tokens_in_pretraining_dataset:\n",
    "                if token not in token_count['all']:\n",
    "                    token_count['all'][token] = -1\n",
    "                    token_count['train'][token] = -1\n",
    "\n",
    "        character_count['all'] = {}\n",
    "        for character in list(character_count['train'].keys()) + list(character_count['valid'].keys()) + list(character_count['test'].keys()) + list(character_count['deploy'].keys()):\n",
    "            character_count['all'][character] = character_count['train'][character] + character_count['valid'][character] + character_count['test'][character] + character_count['deploy'][character]\n",
    "\n",
    "        for character in all_characters_in_pretraining_dataset:\n",
    "            if character not in character_count['all']:\n",
    "                character_count['all'][character] = -1\n",
    "                character_count['train'][character] = -1\n",
    "\n",
    "        for dataset_type in dataset_filepaths.keys():\n",
    "            if self.verbose: print(\"dataset_type: {0}\".format(dataset_type))\n",
    "            if self.verbose: print(\"len(token_count[dataset_type]): {0}\".format(len(token_count[dataset_type])))\n",
    "\n",
    "        label_count['all'] = {}\n",
    "        for character in list(label_count['train'].keys()) + list(label_count['valid'].keys()) + list(label_count['test'].keys()) + list(label_count['deploy'].keys()):\n",
    "            label_count['all'][character] = label_count['train'][character] + label_count['valid'][character] + label_count['test'][character] + label_count['deploy'][character]\n",
    "\n",
    "        token_count['all'] = utils.order_dictionary(token_count['all'], 'value_key', reverse = True)\n",
    "        label_count['all'] = utils.order_dictionary(label_count['all'], 'key', reverse = False)\n",
    "        character_count['all'] = utils.order_dictionary(character_count['all'], 'value', reverse = True)\n",
    "        if self.verbose: print('character_count[\\'all\\']: {0}'.format(character_count['all']))\n",
    "\n",
    "        token_to_index = {}\n",
    "        token_to_index[self.UNK] = self.UNK_TOKEN_INDEX\n",
    "        iteration_number = 0\n",
    "        number_of_unknown_tokens = 0\n",
    "        if self.verbose: print(\"parameters['remap_unknown_tokens_to_unk']: {0}\".format(parameters['remap_unknown_tokens_to_unk']))\n",
    "        if self.verbose: print(\"len(token_count['train'].keys()): {0}\".format(len(token_count['train'].keys())))\n",
    "        for token, count in token_count['all'].items():\n",
    "            if iteration_number == self.UNK_TOKEN_INDEX: iteration_number += 1\n",
    "\n",
    "            if parameters['remap_unknown_tokens_to_unk'] == 1 and \\\n",
    "                (token_count['train'][token] == 0 or \\\n",
    "                parameters['load_only_pretrained_token_embeddings']) and \\\n",
    "                not utils_nlp.is_token_in_pretrained_embeddings(token, token_to_vector, parameters) and \\\n",
    "                token not in all_tokens_in_pretraining_dataset:\n",
    "                if self.verbose: print(\"token: {0}\".format(token))\n",
    "                if self.verbose: print(\"token.lower(): {0}\".format(token.lower()))\n",
    "                if self.verbose: print(\"re.sub('\\d', '0', token.lower()): {0}\".format(re.sub('\\d', '0', token.lower())))\n",
    "                token_to_index[token] =  self.UNK_TOKEN_INDEX\n",
    "                number_of_unknown_tokens += 1\n",
    "                self.tokens_mapped_to_unk.append(token)\n",
    "            else:\n",
    "                token_to_index[token] = iteration_number\n",
    "                iteration_number += 1\n",
    "        if self.verbose: print(\"number_of_unknown_tokens: {0}\".format(number_of_unknown_tokens))\n",
    "\n",
    "        infrequent_token_indices = []\n",
    "        for token, count in token_count['train'].items():\n",
    "            if 0 < count <= remap_to_unk_count_threshold:\n",
    "                infrequent_token_indices.append(token_to_index[token])\n",
    "        if self.verbose: print(\"len(token_count['train']): {0}\".format(len(token_count['train'])))\n",
    "        if self.verbose: print(\"len(infrequent_token_indices): {0}\".format(len(infrequent_token_indices)))\n",
    "\n",
    "        # Ensure that both B- and I- versions exist for each label\n",
    "        labels_without_bio = set()\n",
    "        for label in label_count['all'].keys():\n",
    "            new_label = utils_nlp.remove_bio_from_label_name(label)\n",
    "            labels_without_bio.add(new_label)\n",
    "        for label in labels_without_bio:\n",
    "            if label == 'O':\n",
    "                continue\n",
    "            if parameters['tagging_format'] == 'bioes':\n",
    "                prefixes = ['B-', 'I-', 'E-', 'S-']\n",
    "            else:\n",
    "                prefixes = ['B-', 'I-']\n",
    "            for prefix in prefixes:\n",
    "                l = prefix + label\n",
    "                if l not in label_count['all']:\n",
    "                    label_count['all'][l] = 0\n",
    "        label_count['all'] = utils.order_dictionary(label_count['all'], 'key', reverse = False)\n",
    "\n",
    "        if parameters['use_pretrained_model']:\n",
    "            self.unique_labels = sorted(list(pretraining_dataset.label_to_index.keys()))\n",
    "            # Make sure labels are compatible with the pretraining dataset.\n",
    "            for label in label_count['all']:\n",
    "                if label not in pretraining_dataset.label_to_index:\n",
    "                    raise AssertionError(\"The label {0} does not exist in the pretraining dataset. \".format(label) +\n",
    "                                         \"Please ensure that only the following labels exist in the dataset: {0}\".format(', '.join(self.unique_labels)))\n",
    "            label_to_index = pretraining_dataset.label_to_index.copy()\n",
    "        else:\n",
    "            label_to_index = {}\n",
    "            iteration_number = 0\n",
    "            for label, count in label_count['all'].items():\n",
    "                label_to_index[label] = iteration_number\n",
    "                iteration_number += 1\n",
    "                self.unique_labels.append(label)\n",
    "\n",
    "        if self.verbose: print('self.unique_labels: {0}'.format(self.unique_labels))\n",
    "\n",
    "        character_to_index = {}\n",
    "        iteration_number = 0\n",
    "        for character, count in character_count['all'].items():\n",
    "            if iteration_number == self.PADDING_CHARACTER_INDEX: iteration_number += 1\n",
    "            character_to_index[character] = iteration_number\n",
    "            iteration_number += 1\n",
    "\n",
    "        if self.verbose: print('token_count[\\'train\\'][0:10]: {0}'.format(list(token_count['train'].items())[0:10]))\n",
    "        token_to_index = utils.order_dictionary(token_to_index, 'value', reverse = False)\n",
    "        if self.verbose: print('token_to_index: {0}'.format(token_to_index))\n",
    "        index_to_token = utils.reverse_dictionary(token_to_index)\n",
    "        if parameters['remap_unknown_tokens_to_unk'] == 1: index_to_token[self.UNK_TOKEN_INDEX] = self.UNK\n",
    "        if self.verbose: print('index_to_token: {0}'.format(index_to_token))\n",
    "\n",
    "        if self.verbose: print('label_count[\\'train\\']: {0}'.format(label_count['train']))\n",
    "        label_to_index = utils.order_dictionary(label_to_index, 'value', reverse = False)\n",
    "        if self.verbose: print('label_to_index: {0}'.format(label_to_index))\n",
    "        index_to_label = utils.reverse_dictionary(label_to_index)\n",
    "        if self.verbose: print('index_to_label: {0}'.format(index_to_label))\n",
    "\n",
    "        character_to_index = utils.order_dictionary(character_to_index, 'value', reverse = False)\n",
    "        index_to_character = utils.reverse_dictionary(character_to_index)\n",
    "        if self.verbose: print('character_to_index: {0}'.format(character_to_index))\n",
    "        if self.verbose: print('index_to_character: {0}'.format(index_to_character))\n",
    "\n",
    "\n",
    "        if self.verbose: print('labels[\\'train\\'][0:10]: {0}'.format(labels['train'][0:10]))\n",
    "        if self.verbose: print('tokens[\\'train\\'][0:10]: {0}'.format(tokens['train'][0:10]))\n",
    "\n",
    "        if self.verbose:\n",
    "            # Print sequences of length 1 in train set\n",
    "            for token_sequence, label_sequence in zip(tokens['train'], labels['train']):\n",
    "                if len(label_sequence) == 1 and label_sequence[0] != 'O':\n",
    "                    print(\"{0}\\t{1}\".format(token_sequence[0], label_sequence[0]))\n",
    "\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.index_to_character = index_to_character\n",
    "        self.character_to_index = character_to_index\n",
    "        self.index_to_label = index_to_label\n",
    "        self.label_to_index = label_to_index\n",
    "        if self.verbose: print(\"len(self.token_to_index): {0}\".format(len(self.token_to_index)))\n",
    "        if self.verbose: print(\"len(self.index_to_token): {0}\".format(len(self.index_to_token)))\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "        token_indices, label_indices, character_indices_padded, character_indices, token_lengths, characters, label_vector_indices = self._convert_to_indices(dataset_filepaths.keys())\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        self.label_indices = label_indices\n",
    "        self.character_indices_padded = character_indices_padded\n",
    "        self.character_indices = character_indices\n",
    "        self.token_lengths = token_lengths\n",
    "        self.characters = characters\n",
    "        self.label_vector_indices = label_vector_indices\n",
    "\n",
    "        self.number_of_classes = max(self.index_to_label.keys()) + 1\n",
    "        self.vocabulary_size = max(self.index_to_token.keys()) + 1\n",
    "        self.alphabet_size = max(self.index_to_character.keys()) + 1\n",
    "        if self.verbose: print(\"self.number_of_classes: {0}\".format(self.number_of_classes))\n",
    "        if self.verbose: print(\"self.alphabet_size: {0}\".format(self.alphabet_size))\n",
    "        if self.verbose: print(\"self.vocabulary_size: {0}\".format(self.vocabulary_size))\n",
    "\n",
    "        # unique_labels_of_interest is used to compute F1-scores.\n",
    "        self.unique_labels_of_interest = list(self.unique_labels)\n",
    "        self.unique_labels_of_interest.remove('O')\n",
    "\n",
    "        self.unique_label_indices_of_interest = []\n",
    "        for lab in self.unique_labels_of_interest:\n",
    "            self.unique_label_indices_of_interest.append(label_to_index[lab])\n",
    "\n",
    "        self.infrequent_token_indices = infrequent_token_indices\n",
    "\n",
    "        if self.verbose: print('self.unique_labels_of_interest: {0}'.format(self.unique_labels_of_interest))\n",
    "        if self.verbose: print('self.unique_label_indices_of_interest: {0}'.format(self.unique_label_indices_of_interest))\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('done ({0:.2f} seconds)'.format(elapsed_time))\n",
    "        \n",
    "        return token_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
